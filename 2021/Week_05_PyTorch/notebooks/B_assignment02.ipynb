{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from IPython.display import clear_output\n",
    "import copy\n",
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../pyfiles/\")\n",
    "from util import MinMax, weights_init, cuda2numpy, do_test, plot_confusion_matrix, image_from_output\n",
    "from util_assignment import Dataset_Fashion_MNIST, transform, Flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "---\n",
    "課題は、データセットの実装とモデルの構造定義から構成されています。\n",
    "その中でも後者の課題をこのノートブックで行います。\n",
    "空欄を埋めて提出してください。\n",
    "この課題では授業で用いた [fashion_dataset1](https://drive.google.com/drive/folders/1pX48el14cR6TY6Wor2e1Smo5cDDoQ_MK?usp=sharing)を使うので、ダウンロードしておいてください。\n",
    "\n",
    "---\n",
    "Your assignment includes dataset implementation and model structuring. This notebook contains a form of the model-structuring homework, so please submit this notebook for evaluation. It requires [fashion_dataset1](https://drive.google.com/drive/folders/1pX48el14cR6TY6Wor2e1Smo5cDDoQ_MK?usp=sharing), the same dataset used in the lesson dataset, so please visit and download it.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = [\"T-shirt\", \"Trouser\", \"Dress\", \"Coat\", \"Sandal\", \"Sneaker\", \"Bag\", \"Ankle_boot\"]\n",
    "root = \"../fashion_dataset1/\"\n",
    "classes = range(len(class_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2: Structure Modification\n",
    "---\n",
    "この課題では、新しい仕様書を用いてモデルの構造を変更してもらいます。\n",
    "授業と異なる部分は斜め文字になっています。\n",
    "このセルから下の２つが変更対象で、変更箇所は強調してあります。\n",
    "`01_classification.ipynb`を参考にして頑張ってください。\n",
    "\n",
    "---\n",
    "In this homework, you are tasked with modifying the structure of the model as shown in new instruction below. The changes from the previous instruction are written in an italic style. Two cells are the only cells you need to modify and I underscored the parts you should change or add. Please refer to the lesson notebook `01_classification.ipynb` for some clues.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Instruction\n",
    "---\n",
    "以下のダイアグラムと手順を元に作ってみましょう。\n",
    "\n",
    "<img src=\"./../data/images/cnn.png\" width=\"800\">\n",
    "\n",
    "- モデルは３つのconvolutional blocksと*2つ*の全結合層でできています。\n",
    "- convolutional blocksは畳み込み層と*アベレージ*プーリングレイヤーでできています。\n",
    "- 活性化関数として、Tanhが用いられています。\n",
    "- もちろん、分類タスクなのでsoftmax関数が最後の処理として加えられています。\n",
    "\n",
    "---\n",
    "The instruction is shown below in a form of a diagram and a procedure. \n",
    "\n",
    "<img src=\"./../data/images/cnn.png\" width=\"800\">\n",
    "\n",
    "\n",
    "- the model consists of 3 convolutional blocks and *2* fully connected layers.\n",
    "- each convolutional block has a convolutional layer and a *averaged* pooling layer.\n",
    "- *Tanh* is employed as an activation function.\n",
    "- Of course, this is classification task, so you should use Softmax function in the last computation.\n",
    "\n",
    "\n",
    "the image is from [this site](https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, nch_in, nch_out):\n",
    "        super(ConvolutionalBlock, self).__init__()\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(nch_in, nch_out, kernel_size=3, stride=1, padding=1),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # ------------- put activation function------------------ #\n",
    "            \n",
    "            #------ ------------------------------------------------- #\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_nch, out_nch, nch):\n",
    "        super(Classifier, self).__init__()\n",
    "        \n",
    "        # ------------- define the model ------------------ #\n",
    "        \n",
    "        # ------------------------------------------------- #\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # ------------- define the training process  ------------------ #\n",
    "        \n",
    "        # ------------------------------------------------------------- #\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "次に訓練用とモデル構造用のハイパーパラメタを定義していきましょう。\n",
    "\n",
    "---\n",
    "Some hyperparameters for training are mentioned in the first cell and for the model are shown in the second cell.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch_num = 301\n",
    "lr = 0.001\n",
    "save_parameter = True # do you wanna save parameters ?\n",
    "test_interval = 3 # the interval among validation check\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_nch = 1 # the number of color channels\n",
    "out_nch = len(classes) # the number of output channels, in this case, classes\n",
    "nch = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "最適化関数、損失関数、そしてスケジューラは以下のを用いました。\n",
    "- optimizer: Adam optimizer, デフォルトのパラメタ\n",
    "- scheduler: 学習率が指数関数的に減少するスケジューラ\n",
    "- loss function: Cross Entropy Loss (CELoss)\n",
    "\n",
    "---\n",
    "Some algorithms are employed: optimization, loss function, and scheduler.\n",
    "- optimizer: Adam optimizer with default parameters\n",
    "- scheduler: learning rate scheduler which lowers the lr exponentially\n",
    "- loss function: Cross Entropy Loss (CELoss)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Classifier(in_nch, out_nch, nch)\n",
    "net = net.to(device)\n",
    "net.apply(weights_init)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "データセットを呼び出しましょう。`Dataloader`とはデータをバッチサイズにまとめてくれて、必要であればシャッフルしてくれます。\n",
    "\n",
    "---\n",
    "This is the preparation of the dataset. `Dataloader` combines the individual data to make batch-sized data and it shuffles them if you want.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11200, 2400)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----- adjustable parameters ----- #\n",
    "each_data_num = 2000\n",
    "train_batch = 512\n",
    "val_batch = 512\n",
    "# --------------------------------- #\n",
    "\n",
    "dataset = Dataset_Fashion_MNIST(root, classes, \"train\", transform[\"train\"], each_data_num=each_data_num)\n",
    "dataloader = torch.utils.data.DataLoader(dataset=dataset, batch_size=train_batch, shuffle=True)\n",
    "valset = Dataset_Fashion_MNIST(root, classes, \"val\", transform[\"train\"], each_data_num=each_data_num)\n",
    "valloader = torch.utils.data.DataLoader(dataset=valset, batch_size=val_batch, shuffle=True)\n",
    "len(dataset), len(valset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "---\n",
    "では、訓練してみましょう。\n",
    "このセルにはエポックごとのロスと正解率を出すようにしてます。\n",
    "そして、検証用データにおいての一番いいエポックを記憶するようにしているので、後に呼び出してみましょう。\n",
    "早く訓練を終わらせるためにGPUの使用をおすすめします。\n",
    "\n",
    "---\n",
    "Let's train your model.\n",
    "The notebook shows you the losses and the accuracies in every epoch.\n",
    "What's more, it'll store the best epoch for the validation dataset, so you can call it when the training is finished.\n",
    "I highly recommend to use your GPU to quicken the process.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.train()\n",
    "losses = []\n",
    "losses_epoch = []\n",
    "accs = []\n",
    "acc_epoch = []\n",
    "acc_test_list = []\n",
    "best_epoch = 0\n",
    "best_acc = 0\n",
    "for epoch in range(epoch_num):\n",
    "    for itr, data in enumerate(dataloader):\n",
    "        net.train()\n",
    "        x = data[0].to(device)\n",
    "        label = data[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y = net(x)\n",
    "        loss = criterion(y, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss = cuda2numpy(loss)\n",
    "        label = cuda2numpy(label)\n",
    "        y = cuda2numpy(y)\n",
    "        acc = (np.argmax(y, axis=1) == label).sum().item()/len(label)\n",
    "        losses.append(loss)\n",
    "        accs.append(acc)\n",
    "    scheduler.step()\n",
    "    losses_epoch.append(np.mean(np.array(losses)))\n",
    "    acc_epoch.append(np.mean(np.array(accs)))\n",
    "    if epoch%test_interval==0:\n",
    "        labels, outputs = do_test(net, valloader, device, \"eval\")\n",
    "        pred_labels = np.argmax(outputs, axis=1) \n",
    "        acc_test = accuracy_score(labels, pred_labels)\n",
    "        acc_test_list.append(acc_test)\n",
    "        if best_acc < acc_test:\n",
    "            best_acc = acc_test\n",
    "            best_epoch = epoch\n",
    "            \n",
    "    clear_output(wait=True)\n",
    "    print(epoch)\n",
    "    print(f\"test acc in {epoch//test_interval*test_interval} : {acc_test}\")\n",
    "    print(f\"best acc is   : {best_acc} in epoch{best_epoch}\")\n",
    "    \n",
    "    if save_parameter:\n",
    "        if epoch%test_interval==0:\n",
    "            torch.save(net.state_dict(), \n",
    "                       f\"../instant_parameters/classifier_assignment_nch{nch}_lr{lr}_epoch{epoch}.pth\")\n",
    "    fig = plt.figure(figsize=(10,5))\n",
    "    ax = fig.add_subplot(1,2,1)\n",
    "    ax.plot(losses_epoch)\n",
    "    ax.set_title(\"loss\")\n",
    "    ax = fig.add_subplot(1,2,2)\n",
    "    ax.plot(acc_epoch, label=\"train\")\n",
    "    ax.plot(np.arange(epoch//test_interval+1)*test_interval, acc_test_list, label=\"val\")\n",
    "    ax.legend()\n",
    "    ax.set_title('accuracy')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result\n",
    "---\n",
    "では出力結果をみて、一番いいエポックを確認してみましょう。\n",
    "そのあと、`load_epoch`に代入してください。\n",
    "\n",
    "---\n",
    "Ok, next, please look at the output of the last cell and note the best epoch as illustrated below. \n",
    "And put the epoch into `load_epoch` in the cell below.\n",
    "\n",
    "<img src=\"./../data/images/result.png\" width=\"600\">\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- adjustable parameters ----- #\n",
    "load_epoch = \n",
    "# --------------------------------- #\n",
    "\n",
    "load_parameter = True\n",
    "if load_parameter:\n",
    "    model_path = f\"../instant_parameters/classifier_assignment_nch16_lr0.001_epoch{load_epoch}.pth\"\n",
    "    model = torch.load(model_path)\n",
    "    net.load_state_dict(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2400"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----- adjustable parameters ----- #\n",
    "test_batch = 512\n",
    "# --------------------------------- #\n",
    "\n",
    "testset = Dataset_Fashion_MNIST(root, classes, \"test\", transform[\"test\"], each_data_num=each_data_num)\n",
    "testloader = torch.utils.data.DataLoader(dataset=testset, batch_size=train_batch, shuffle=False)\n",
    "len(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test acc is : 0.9479166666666666\n"
     ]
    }
   ],
   "source": [
    "# ----- adjustable parameters ----- #\n",
    "test_num = 10\n",
    "# --------------------------------- #\n",
    "\n",
    "acc_list = []\n",
    "cm_list = []\n",
    "for i in range(test_num):\n",
    "    print(i)\n",
    "    labels, outputs = do_test(net, testloader, device, \"eval\")\n",
    "    pred_labels = np.argmax(outputs, axis=1) \n",
    "    acc_test = accuracy_score(labels, pred_labels)\n",
    "    acc_list.append(acc_test)\n",
    "    cm = confusion_matrix(labels, pred_labels, labels=np.arange(len(classes)))\n",
    "    cm_list.append(cm)\n",
    "    clear_output(wait=True)\n",
    "print(f\"test acc is : {np.mean(acc_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_idx_lbl = class_labels.copy()\n",
    "for i in range(len(class_labels)):\n",
    "    class_idx_lbl[i] = f\"{i}:{class_labels[i]}\"\n",
    "        \n",
    "cm = np.mean(np.array(cm_list), axis=0)\n",
    "plot_confusion_matrix(cm, class_idx_lbl, normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misclassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- adjustable parameters ----- #\n",
    "width = 8\n",
    "length = 4\n",
    "# --------------------------------- #\n",
    "\n",
    "index_bool = pred_labels!=labels\n",
    "indexes = np.random.choice(np.arange(len(labels))[index_bool], size=min(width*length,index_bool.sum()), replace=False)\n",
    "\n",
    "fig = plt.figure(figsize=(4*width, 5*length))\n",
    "for i in range(len(indexes)):\n",
    "    idx = indexes[i]\n",
    "    ax = fig.add_subplot(length, width, i+1)\n",
    "    x, label = testset[idx]\n",
    "    ax.imshow(image_from_output(x[np.newaxis])[0])\n",
    "    title = f\"true:{class_labels[int(labels[idx])]} \\n pred:{class_labels[pred_labels[idx]]}\"\n",
    "    ax.set_title(title, fontsize=15)\n",
    "    plt.axis('off')\n",
    "plt.subplots_adjust(wspace=0.01, hspace=0.03)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please run the code until this cell\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
