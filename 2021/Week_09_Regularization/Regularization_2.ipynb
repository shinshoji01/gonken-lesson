{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review of homework\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularizers\n",
    "\n",
    "The function of regularizers is to improve generalization.  \n",
    "They create restrictions on the values that weights can take, and reduce the search space that the optimizer explores.  \n",
    "As such, regularizers **decrease model capacity**. This is the reason for their capacity to reduce overfitting.\n",
    "\n",
    "Let's look at five common regularization techniques, why we use them, and how they help prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### New framework\n",
    "*Why are we changing the weights*: Belief  \n",
    "*What does it change*: Prior  \n",
    "*How do we change it*: Practical modification  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as vF\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import os\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import PIL.Image as Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_MNIST(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, classes, mode=\"train\", transform=None, balance=[0.7,0.15,0.15], each_data_num=10000000):\n",
    "        \n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "\n",
    "        images = {} \n",
    "        labels = {}\n",
    "        \n",
    "        for cl in classes:\n",
    "            path_list = glob.glob(root + f\"{cl}/*\")\n",
    "            path_list.sort()\n",
    "            path_list = path_list[:each_data_num]\n",
    "            train_num = int(balance[0]*len(path_list))\n",
    "            val_num = int(balance[1]*len(path_list))\n",
    "            test_num = int(balance[2]*len(path_list))\n",
    "            if mode==\"train\":\n",
    "                path_list = path_list[:train_num]\n",
    "            elif mode==\"val\":\n",
    "                path_list = path_list[train_num:train_num+val_num]\n",
    "            elif mode==\"test\":\n",
    "                path_list = path_list[-test_num:]\n",
    "            images[str(cl)] = path_list\n",
    "            labels[str(cl)] = [cl]*len(path_list)\n",
    "            \n",
    "        # combine them together\n",
    "        for label in classes:\n",
    "            for image, label in zip(images[str(label)], labels[str(label)]):\n",
    "                self.images.append(image)\n",
    "                self.labels.append(label)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        image = self.images[index]\n",
    "        label = self.labels[index]\n",
    "        \n",
    "        with open(image, 'rb') as f:\n",
    "            image = Image.open(f)\n",
    "            image = image.convert(\"L\")\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPOCH = 100\n",
    "LR = 0.01\n",
    "TRIALS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BasicModel, self).__init__()\n",
    "        self.c1 = nn.Conv2d(1, 9, (3,3), padding=(1,1))\n",
    "        self.p1 = nn.MaxPool2d(2, stride=2)\n",
    "        self.c2 = nn.Conv2d(9, 16, (3,3), padding=(1,1))\n",
    "        self.p2 = nn.MaxPool2d(2, stride=2)\n",
    "        self.l1 = nn.Linear(7*7*16, 32)\n",
    "        self.l2 = nn.Linear(32, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.c1(x))\n",
    "        h = self.p1(h)\n",
    "        h = F.relu(self.c2(h))\n",
    "        h = self.p2(h)\n",
    "        h = h.view(-1, 7*7*16)\n",
    "        h = F.relu(self.l1(h))\n",
    "        h = F.relu(self.l2(h))\n",
    "        y = F.softmax(h, dim=1)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = Dataset_MNIST('./smalldataset/', mode='train', classes=[5,6,8], transform=torchvision.transforms.ToTensor(), balance=[0.8,0,0.2])\n",
    "dataset_test = Dataset_MNIST('./smalldataset/', mode='test', classes=[5,6,8], transform=torchvision.transforms.ToTensor(), balance=[0.8,0,0.2])\n",
    "dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_train_acc():\n",
    "    c = 0 \n",
    "    w = 0\n",
    "    for x, y in dataset_train:\n",
    "        if model(x[None,...].cuda()).argmax()==y:\n",
    "            c += 1\n",
    "        else:\n",
    "            w -= -1\n",
    "#     print('Train accuracy: {}'.format(c/(c+w)))\n",
    "    return c/(c+w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_test_acc():\n",
    "    c = 0 \n",
    "    w = 0\n",
    "    for x, y in dataset_test:\n",
    "        if model(x[None,...].cuda()).argmax()==y:\n",
    "            c += 1\n",
    "        else:\n",
    "            w -= -1\n",
    "#     print('Test accuracy: {}'.format(c/(c+w)))\n",
    "    return c/(c+w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial: 0, Epoch: 99, Loss: 1.6925469636917114\n",
      "Trial: 1, Epoch: 99, Loss: 1.5728302001953125\n",
      "Trial: 2, Epoch: 99, Loss: 1.9711151123046875\n",
      "Trial: 3, Epoch: 99, Loss: 2.3736147880554286\n",
      "Trial: 4, Epoch: 99, Loss: 2.3235127925872803\n",
      "Trial: 5, Epoch: 99, Loss: 1.7843334674835205\n",
      "Trial: 6, Epoch: 99, Loss: 2.2456319332122803\n",
      "Trial: 7, Epoch: 99, Loss: 2.3176929950714118\n",
      "Trial: 8, Epoch: 99, Loss: 1.7755613327026367\n",
      "Trial: 9, Epoch: 99, Loss: 1.8830586671829224\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "for trial in range(TRIALS):\n",
    "    criterion = nn.CrossEntropyLoss().cuda()\n",
    "    model = BasicModel().cuda()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=LR, momentum=0.9)\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda epoch: 0.999**epoch)\n",
    "    \n",
    "    weights = []\n",
    "    for child in model.children():\n",
    "        try:\n",
    "            weights.append(child.weight)\n",
    "            weights.append(child.bias)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    best_loss = 1000\n",
    "    for epoch in range(MAX_EPOCH):\n",
    "        for x, y in dataloader_train:\n",
    "    #         zero buffers\n",
    "            optimizer.zero_grad() \n",
    "    #         forward propagation\n",
    "            prediction = model(x.cuda())\n",
    "\n",
    "            loss = criterion(prediction, y.cuda())+sum([torch.abs(w).sum() for w in weights])*0.001\n",
    "\n",
    "    #         gradient calculation\n",
    "            loss.backward()\n",
    "    #         parameter update\n",
    "            optimizer.step() \n",
    "\n",
    "        print('Trial: {}, Epoch: {}, Loss: {}'.format(trial, epoch, loss), end='\\r')\n",
    "        scheduler.step()\n",
    "        \n",
    "        if loss<best_loss:\n",
    "            best_loss = loss\n",
    "            \n",
    "    losses.append(loss)\n",
    "    train_acc.append(calc_train_acc())\n",
    "    test_acc.append(calc_test_acc())\n",
    "    print()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Loss: 1.994, Mean Train Acc: 0.5639, Mean Test Acc: 0.5111\n"
     ]
    }
   ],
   "source": [
    "print('Mean Loss: {:.4}, Mean Train Acc: {:.4}, Mean Test Acc: {:.4}'.format(\n",
    "                                                        sum(losses)/TRIALS,\n",
    "                                                        sum(train_acc)/TRIALS,\n",
    "                                                        sum(test_acc)/TRIALS,\n",
    "                                                        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## L1 regularizer\n",
    "\n",
    "*Why*: weights should be sparse  \n",
    "*What*: pull less important weights towards 0  \n",
    "*How*: L1 penalty to the weights (sum of absolute weights)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial: 0, Epoch: 99, Loss: 2.0159039497375493\n",
      "Trial: 1, Epoch: 99, Loss: 1.8974485397338867\n",
      "Trial: 2, Epoch: 99, Loss: 1.5526751279830933\n",
      "Trial: 3, Epoch: 99, Loss: 2.3704555034637452\n",
      "Trial: 4, Epoch: 99, Loss: 1.9689602851867676\n",
      "Trial: 5, Epoch: 99, Loss: 2.3221611976623535\n",
      "Trial: 6, Epoch: 99, Loss: 1.6094920635223389\n",
      "Trial: 7, Epoch: 99, Loss: 1.7936595678329468\n",
      "Trial: 8, Epoch: 99, Loss: 1.7900675535202026\n",
      "Trial: 9, Epoch: 99, Loss: 1.5746560096740723\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "for trial in range(TRIALS):\n",
    "    criterion = nn.CrossEntropyLoss().cuda()\n",
    "    model = BasicModel().cuda()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=LR, momentum=0.9)\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda epoch: 0.999**epoch)\n",
    "    \n",
    "    weights = []\n",
    "    for child in model.children():\n",
    "        try:\n",
    "            weights.append(child.weight)\n",
    "            weights.append(child.bias)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    best_loss = 1000\n",
    "    for epoch in range(MAX_EPOCH):\n",
    "        for x, y in dataloader_train:\n",
    "    #         zero buffers\n",
    "            optimizer.zero_grad() \n",
    "    #         forward propagation\n",
    "            prediction = model(x.cuda())\n",
    "\n",
    "            ###########################################\n",
    "            loss = criterion(prediction, y.cuda())+sum([torch.abs(w).sum() for w in weights])*0.001\n",
    "            ###########################################\n",
    "\n",
    "    #         gradient calculation\n",
    "            loss.backward()\n",
    "    #         parameter update\n",
    "            optimizer.step() \n",
    "\n",
    "        print('Trial: {}, Epoch: {}, Loss: {}'.format(trial, epoch, loss), end='\\r')\n",
    "        scheduler.step()\n",
    "        \n",
    "        if loss<best_loss:\n",
    "            best_loss = loss\n",
    "            \n",
    "    losses.append(loss)\n",
    "    train_acc.append(calc_train_acc())\n",
    "    test_acc.append(calc_test_acc())\n",
    "    print()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Loss: 1.89, Mean Train Acc: 0.6944, Mean Test Acc: 0.6167\n"
     ]
    }
   ],
   "source": [
    "print('Mean Loss: {:.4}, Mean Train Acc: {:.4}, Mean Test Acc: {:.4}'.format(\n",
    "                                                        sum(losses)/TRIALS,\n",
    "                                                        sum(train_acc)/TRIALS,\n",
    "                                                        sum(test_acc)/TRIALS,\n",
    "                                                        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## L2 regularizer\n",
    "\n",
    "*Why*: weights should be smaller evenly  \n",
    "*What*: pull all weights evenly towards the origin (0, 0, ..., 0)  \n",
    "*How*: L2 penalty to the weights (sum of square weights)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial: 0, Epoch: 99, Loss: 2.3579778671264655\n",
      "Trial: 1, Epoch: 99, Loss: 2.3097934722900398\n",
      "Trial: 2, Epoch: 99, Loss: 1.8985996246337893\n",
      "Trial: 3, Epoch: 99, Loss: 1.8908604383468628\n",
      "Trial: 4, Epoch: 99, Loss: 1.5808395147323608\n",
      "Trial: 5, Epoch: 99, Loss: 1.6338837146759033\n",
      "Trial: 6, Epoch: 99, Loss: 1.5922881364822388\n",
      "Trial: 7, Epoch: 99, Loss: 2.1117160320281982\n",
      "Trial: 8, Epoch: 99, Loss: 2.2828524112701416\n",
      "Trial: 9, Epoch: 99, Loss: 1.7834382057189941\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "for trial in range(TRIALS):\n",
    "    criterion = nn.CrossEntropyLoss().cuda()\n",
    "    model = BasicModel().cuda()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=LR, momentum=0.9)\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda epoch: 0.999**epoch)\n",
    "    \n",
    "    weights = []\n",
    "    for child in model.children():\n",
    "        try:\n",
    "            weights.append(child.weight)\n",
    "            weights.append(child.bias)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    best_loss = 1000\n",
    "    for epoch in range(MAX_EPOCH):\n",
    "        for x, y in dataloader_train:\n",
    "    #         zero buffers\n",
    "            optimizer.zero_grad() \n",
    "    #         forward propagation\n",
    "            prediction = model(x.cuda())\n",
    "\n",
    "            ###########################################\n",
    "            loss = criterion(prediction, y.cuda())+sum([(w**2).sum() for w in weights])*0.01\n",
    "            ###########################################\n",
    "\n",
    "    #         gradient calculation\n",
    "            loss.backward()\n",
    "    #         parameter update\n",
    "            optimizer.step() \n",
    "\n",
    "        print('Trial: {}, Epoch: {}, Loss: {}'.format(trial, epoch, loss), end='\\r')\n",
    "        scheduler.step()\n",
    "        \n",
    "        if loss<best_loss:\n",
    "            best_loss = loss\n",
    "            \n",
    "    losses.append(loss)\n",
    "    train_acc.append(calc_train_acc())\n",
    "    test_acc.append(calc_test_acc())\n",
    "    print()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Loss: 1.944, Mean Train Acc: 0.6389, Mean Test Acc: 0.5889\n"
     ]
    }
   ],
   "source": [
    "print('Mean Loss: {:.4}, Mean Train Acc: {:.4}, Mean Test Acc: {:.4}'.format(\n",
    "                                                        sum(losses)/TRIALS,\n",
    "                                                        sum(train_acc)/TRIALS,\n",
    "                                                        sum(test_acc)/TRIALS,\n",
    "                                                        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Early stopping:\n",
    "*Why*: when the model starts overfitting, the training loss will continue to decrease but the validation loss will not  \n",
    "*What*: reduce the probability of weights that act in this regime  \n",
    "*How*: stop training when validation loss stops improving  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATIENCE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial: 0, Epoch: 99, Loss: 1.4647108316421509\n",
      "Trial: 1, Epoch: 99, Loss: 1.4616041183471683\n",
      "Trial: 2, Epoch: 99, Loss: 2.0824708938598633\n",
      "Trial: 3, Epoch: 99, Loss: 1.8164957761764526\n",
      "Trial: 4, Epoch: 99, Loss: 1.8316044807434082\n",
      "Trial: 5, Epoch: 99, Loss: 1.7113192081451416\n",
      "Trial: 6, Epoch: 99, Loss: 1.5884190797805786\n",
      "Trial: 7, Epoch: 99, Loss: 1.4613454341888428\n",
      "Trial: 8, Epoch: 99, Loss: 1.4613285064697266\n",
      "Trial: 9, Epoch: 99, Loss: 1.4629142284393314\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "\n",
    "for trial in range(TRIALS):\n",
    "    criterion = nn.CrossEntropyLoss().cuda()\n",
    "    model = BasicModel().cuda()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=LR, momentum=0.9)\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda epoch: 0.999**epoch)\n",
    "    \n",
    "    \n",
    "    weights = []\n",
    "    for child in model.children():\n",
    "        try:\n",
    "            weights.append(child.weight)\n",
    "            weights.append(child.bias)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    best_loss = 1000\n",
    "    for epoch in range(MAX_EPOCH):\n",
    "        \n",
    "        ###########################################\n",
    "        no_improvement = 0\n",
    "        best_loss = 1000\n",
    "        ###########################################\n",
    "    \n",
    "        for x, y in dataloader_train:\n",
    "    #         zero buffers\n",
    "            optimizer.zero_grad() \n",
    "    #         forward propagation\n",
    "            prediction = model(x.cuda())\n",
    "\n",
    "            loss = criterion(prediction, y.cuda())\n",
    "\n",
    "    #         gradient calculation\n",
    "            loss.backward()\n",
    "    #         parameter update\n",
    "            optimizer.step() \n",
    "        \n",
    "            if loss<best_loss:\n",
    "                best_loss=loss\n",
    "                no_improvement = 0\n",
    "            else:\n",
    "                no_improvement += 1\n",
    "                if no_improvement>PATIENCE:\n",
    "                    break\n",
    "\n",
    "        print('Trial: {}, Epoch: {}, Loss: {}'.format(trial, epoch, loss), end='\\r')\n",
    "        scheduler.step()\n",
    "        \n",
    "        if loss<best_loss:\n",
    "            best_loss = loss\n",
    "            \n",
    "    losses.append(loss)\n",
    "    train_acc.append(calc_train_acc())\n",
    "    test_acc.append(calc_test_acc())\n",
    "    print()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Loss: 1.634, Mean Train Acc: 0.7597, Mean Test Acc: 0.6722\n"
     ]
    }
   ],
   "source": [
    "print('Mean Loss: {:.4}, Mean Train Acc: {:.4}, Mean Test Acc: {:.4}'.format(\n",
    "                                                        sum(losses)/TRIALS,\n",
    "                                                        sum(train_acc)/TRIALS,\n",
    "                                                        sum(test_acc)/TRIALS,\n",
    "                                                        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Dataset enhancement\n",
    "*Why*: by increasing our dataset we can reduce overfitting, and there are easy ways to generate new training pairs from existing pairs  \n",
    "*What*: reduce the probability of weights that perform badly in the specific ways for which we enhanced for  \n",
    "*How*: by making making changes to existing examples in ways to which we believe the model should be invariant  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.RandomCrop((20,20)),\n",
    "    torchvision.transforms.Pad((4,4)),\n",
    "    torchvision.transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train_enhanced = copy.copy(dataset_train)\n",
    "dataset_train_enhanced.transform = new_transform\n",
    "dataloader_train_enhanced = torch.utils.data.DataLoader(dataset_train_enhanced, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x16d80978cd0>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(dataset_train_enhanced[0][0].numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial: 0, Epoch: 99, Loss: 1.5213726758956914\n",
      "Trial: 1, Epoch: 99, Loss: 2.3025853633880615\n",
      "Trial: 2, Epoch: 99, Loss: 1.8438680171966553\n",
      "Trial: 3, Epoch: 99, Loss: 1.7129114866256714\n",
      "Trial: 4, Epoch: 99, Loss: 2.1256837844848633\n",
      "Trial: 5, Epoch: 99, Loss: 1.7659256458282478\n",
      "Trial: 6, Epoch: 99, Loss: 1.9604092836380005\n",
      "Trial: 7, Epoch: 99, Loss: 1.9906835556030273\n",
      "Trial: 8, Epoch: 99, Loss: 1.6827559471130375\n",
      "Trial: 9, Epoch: 99, Loss: 2.0861489772796636\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "for trial in range(TRIALS):\n",
    "    criterion = nn.CrossEntropyLoss().cuda()\n",
    "    model = BasicModel().cuda()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=LR, momentum=0.9)\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda epoch: 0.999**epoch)\n",
    "    \n",
    "    weights = []\n",
    "    for child in model.children():\n",
    "        try:\n",
    "            weights.append(child.weight)\n",
    "            weights.append(child.bias)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    best_loss = 1000\n",
    "    for epoch in range(MAX_EPOCH):\n",
    "        for x, y in dataloader_train_enhanced:\n",
    "    #         zero buffers\n",
    "            optimizer.zero_grad() \n",
    "    #         forward propagation\n",
    "            prediction = model(x.cuda())\n",
    "\n",
    "            loss = criterion(prediction, y.cuda())\n",
    "\n",
    "    #         gradient calculation\n",
    "            loss.backward()\n",
    "    #         parameter update\n",
    "            optimizer.step() \n",
    "\n",
    "        print('Trial: {}, Epoch: {}, Loss: {}'.format(trial, epoch, loss), end='\\r')\n",
    "        scheduler.step()\n",
    "        \n",
    "        if loss<best_loss:\n",
    "            best_loss = loss\n",
    "            \n",
    "    losses.append(loss)\n",
    "    train_acc.append(calc_train_acc())\n",
    "    test_acc.append(calc_test_acc())\n",
    "    print()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Loss: 1.899, Mean Train Acc: 0.6403, Mean Test Acc: 0.6278\n"
     ]
    }
   ],
   "source": [
    "print('Mean Loss: {:.4}, Mean Train Acc: {:.4}, Mean Test Acc: {:.4}'.format(\n",
    "                                                        sum(losses)/TRIALS,\n",
    "                                                        sum(train_acc)/TRIALS,\n",
    "                                                        sum(test_acc)/TRIALS,\n",
    "                                                        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Dropout\n",
    "*Why*: machine learning methods generalize better if they have some redundancy (think bagging methods), always training together reduces the individual power of each weight  \n",
    "*What*: Reduce the probability of weights that produce very inter-dependent neurons (increase the probability of independent feature extraction)\n",
    "*How*: at each training step, prevent randomly selected weights from participating in any calculations (both forwards and backwards propagation)  \n",
    "  \n",
    "Disclaimer: We don't really understand why dropout works so well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropoutModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DropoutModel, self).__init__()\n",
    "        self.c1 = nn.Conv2d(1, 9, (3,3), padding=(1,1))\n",
    "        self.p1 = nn.MaxPool2d(2, stride=2)\n",
    "        \n",
    "        ###########################################\n",
    "        self.dr1 = nn.Dropout2d(0.3)\n",
    "        ###########################################\n",
    "       \n",
    "        self.c2 = nn.Conv2d(9, 16, (3,3), padding=(1,1))\n",
    "        self.p2 = nn.MaxPool2d(2, stride=2)\n",
    "         \n",
    "        ###########################################\n",
    "        self.dr2 = nn.Dropout2d(0.3)\n",
    "        ###########################################\n",
    "        \n",
    "        self.l1 = nn.Linear(7*7*16, 32)\n",
    "        self.l2 = nn.Linear(32, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.c1(x))\n",
    "        \n",
    "        ###########################################\n",
    "        h = self.dr1(h)\n",
    "        ###########################################\n",
    "        \n",
    "        h = self.p1(h)\n",
    "        h = F.relu(self.c2(h))\n",
    "        \n",
    "        ###########################################\n",
    "        h = self.dr2(h)\n",
    "        ###########################################\n",
    "        \n",
    "        h = self.p2(h)\n",
    "        h = h.view(-1, 7*7*16)\n",
    "        h = F.relu(self.l1(h))\n",
    "        h = F.relu(self.l2(h))\n",
    "        y = F.softmax(h, dim=1)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial: 0, Epoch: 99, Loss: 2.2110929489135744\n",
      "Trial: 1, Epoch: 99, Loss: 1.9611510038375854\n",
      "Trial: 2, Epoch: 99, Loss: 1.4865503311157227\n",
      "Trial: 3, Epoch: 99, Loss: 1.8488260507583618\n",
      "Trial: 4, Epoch: 99, Loss: 2.3026018142700195\n",
      "Trial: 5, Epoch: 99, Loss: 1.4611811637878418\n",
      "Trial: 6, Epoch: 99, Loss: 1.5831022262573242\n",
      "Trial: 7, Epoch: 99, Loss: 2.2110857963562016\n",
      "Trial: 8, Epoch: 99, Loss: 1.5928748846054077\n",
      "Trial: 9, Epoch: 99, Loss: 1.6543922424316406\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "for trial in range(TRIALS):\n",
    "    criterion = nn.CrossEntropyLoss().cuda()\n",
    "    model = DropoutModel().cuda()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=LR, momentum=0.9)\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda epoch: 0.999**epoch)\n",
    "    \n",
    "    weights = []\n",
    "    for child in model.children():\n",
    "        try:\n",
    "            weights.append(child.weight)\n",
    "            weights.append(child.bias)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    best_loss = 1000\n",
    "    for epoch in range(MAX_EPOCH):\n",
    "        for x, y in dataloader_train:\n",
    "    #         zero buffers\n",
    "            optimizer.zero_grad() \n",
    "    #         forward propagation\n",
    "            prediction = model(x.cuda())\n",
    "\n",
    "            loss = criterion(prediction, y.cuda())\n",
    "\n",
    "    #         gradient calculation\n",
    "            loss.backward()\n",
    "    #         parameter update\n",
    "            optimizer.step() \n",
    "\n",
    "        print('Trial: {}, Epoch: {}, Loss: {}'.format(trial, epoch, loss), end='\\r')\n",
    "        scheduler.step()\n",
    "        \n",
    "        if loss<best_loss:\n",
    "            best_loss = loss\n",
    "            \n",
    "    losses.append(loss)\n",
    "    train_acc.append(calc_train_acc())\n",
    "    test_acc.append(calc_test_acc())\n",
    "    print()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Loss: 1.831, Mean Train Acc: 0.5806, Mean Test Acc: 0.5333\n"
     ]
    }
   ],
   "source": [
    "print('Mean Loss: {:.4}, Mean Train Acc: {:.4}, Mean Test Acc: {:.4}'.format(\n",
    "                                                        sum(losses)/TRIALS,\n",
    "                                                        sum(train_acc)/TRIALS,\n",
    "                                                        sum(test_acc)/TRIALS,\n",
    "                                                        ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
