{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A review of statistics\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the equation for conditional probabilities:  \n",
    "\n",
    "$ P(X,Y) = P(X|Y) * P(Y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ P(X|Y) * P(Y) = P(Y|X) * P(X)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ P(X|Y) = \\frac{P(Y|X) * P(X)}{P(Y)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Bayes' Law\n",
    "\n",
    "This equation is known as Bayes' law. It is the starting point for an entire field of study called Bayesian statistics, but here we will focus on this core principle.  \n",
    "\n",
    "Here, $P(X)$ is called the **prior distribution** and $P(X|Y)$ the **posterior distribution**. You can think of the prior as what we originally believed the distribution of X to be, and the posterior our new beliefs afterseeing Y.\n",
    "\n",
    "Consider a real-life example: you are arriving at New York for the first time, coming out of the airport. Searching for a taxi, you remember that in every movie you've ever seen, taxis in New York are always yellow. This is your **prior** distribution: all taxis are yellow in New York. However, soon after leaving the airport, you see a black taxi. You must now update your belief about taxis in New York. Therefore, you arrive at a new distribution: perhaps not **all** taxis are yellow. This is your **posterior** after seeing a black taxi.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Machine learning is statistics\n",
    "\n",
    "What does this have to do with machine learning, I hear you ask. Well, training a model is a random process, and for a given dataset, we have a probability distribution for the values of our parameters!  \n",
    "Let's call this probability distribution $p(\\theta|X)$, that is, the joint probability distribution of over our model parameters given set of training data X.  \n",
    "\n",
    "This gives us:\n",
    "$ p(\\theta|X) = \\frac{p(X|\\theta) * p(\\theta)}{p(X)}$\n",
    "  \n",
    "Let us consider the base case, where we make no assumptions about our parameters. Our prior $p(\\theta)$ is therefore constant, and so $p(\\theta|X)$ will be high where $p(X|\\theta)$ is high. That is, the model that best describes our data is most likely to emerge.   \n",
    "Take a moment to make sure you understand why this is.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Priors push our results in different directions\n",
    "\n",
    "Since our prior represents our initial beliefs, we can use it to represent our assumptions about properties of our parameters $\\theta$.\n",
    "For instance, if we assume that our weights should be larger in the first layer than in the second (for whatever reason), we can represent this as a larger probability density for weight matrices that represent this belief.\n",
    "This changes the distribution $p(\\theta)$, but neither $p(X|\\theta)$ nor $p(X)$. Can you see how this should change our posterior?\n",
    "\n",
    "By using a prior, we bias our training algorithm to more likely converge to some parameters rather than others. It improves the probability that parameters that agree with our assumptions will emerge, and decreases the probability of those that don't.  \n",
    "In short, it **manipulates our results to behave in a certain way**.\n",
    "\n",
    "\n",
    "### Exercise 1:\n",
    "What beliefs about the parameters could you imagine being reasonable for a neural network (perhaps for a specific application)? If you can think of one, how could they be represented as a prior?\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Working together with the algorithm\n",
    "\n",
    "Why bother with prior in the first place though? Isn't it easier to just let the training algorithm find what's best without our input?  \n",
    "\n",
    "Not for the training algorithm! \n",
    "By giving it a prior, you essentially are helping the training algorithm by telling it where to look!  \n",
    "\n",
    "Think of it this way: the training algorithm is a *heuristic* algorithm, that is to say, its job is to look for something--the best values for $\\theta$.\n",
    "Imagine you lost your phone. You can either assume nothing and search every corner of the house, or remember the places you've been and assume that most likely it's somewhere nearby.  \n",
    "Which is easier?  \n",
    "\n",
    "Prior can do the same for our optimization process: it tells it where to start looking for the best values.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Priors can be manipulated\n",
    "\n",
    "In machine learning, prior also have the function of telling the optimization process what properties we want from our results.  \n",
    "By increasing the probability of parameters that give desirable results, we can bias our algorithm to choose those over other alternatives!\n",
    "\n",
    "For instance, by using a prior that gives high probability to solutions that generalize well, we can decrease the probability of ending up with an overfitted model!\n",
    "\n",
    "Let's look at a concrete example. \n",
    "The initial values of our model are where out algorithm begins searching for optimal parameters. Initially, this increases the probability of values near those starting points, since we update using small steps.   \n",
    "The longer we train, the further away our parameters can go. Somehow, our prior is getting weaker and weaker. We can think of this as our model gradually giving more and more importance to what it learns from the data rather than the prior.\n",
    "This makes the effect of this prior weaker and weaker the longer we train! \n",
    "While the specific reason for this decay is beyond the scope of this lesson, note that not all priors behave like this.\n",
    "\n",
    "\n",
    "### Exercise 2:\n",
    "Can you think of other concrete ways that we might insert our beliefs into our model or our training process?  \n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## Priors in neural networks\n",
    "\n",
    "\n",
    "### Exercise 3:\n",
    "Please try to apply modify the following model so that it gives better results on average.\n",
    "Whatever modifications you make, please justify them, if possible, in terms of Bayesian priors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as vF\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import os\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import PIL.Image as Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_MNIST(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, classes, mode=\"train\", transform=None, balance=[0.7,0.15,0.15], each_data_num=10000000):\n",
    "        \n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "\n",
    "        images = {} \n",
    "        labels = {}\n",
    "        \n",
    "        for cl in classes:\n",
    "            path_list = glob.glob(root + f\"{cl}/*\")\n",
    "            path_list.sort()\n",
    "            path_list = path_list[:each_data_num]\n",
    "            train_num = int(balance[0]*len(path_list))\n",
    "            val_num = int(balance[1]*len(path_list))\n",
    "            test_num = int(balance[2]*len(path_list))\n",
    "            if mode==\"train\":\n",
    "                path_list = path_list[:train_num]\n",
    "            elif mode==\"val\":\n",
    "                path_list = path_list[train_num:train_num+val_num]\n",
    "            elif mode==\"test\":\n",
    "                path_list = path_list[-test_num:]\n",
    "            images[str(cl)] = path_list\n",
    "            labels[str(cl)] = [cl]*len(path_list)\n",
    "            \n",
    "        # combine them together\n",
    "        for label in classes:\n",
    "            for image, label in zip(images[str(label)], labels[str(label)]):\n",
    "                self.images.append(image)\n",
    "                self.labels.append(label)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        image = self.images[index]\n",
    "        label = self.labels[index]\n",
    "        \n",
    "        with open(image, 'rb') as f:\n",
    "            image = Image.open(f)\n",
    "            image = image.convert(\"L\")\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPOCH = 100\n",
    "LR = 0.01\n",
    "TRIALS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BasicModel, self).__init__()\n",
    "        self.c1 = nn.Conv2d(1, 9, (3,3), padding=(1,1))\n",
    "        self.p1 = nn.MaxPool2d(2, stride=2)\n",
    "        self.c2 = nn.Conv2d(9, 16, (3,3), padding=(1,1))\n",
    "        self.p2 = nn.MaxPool2d(2, stride=2)\n",
    "        self.l1 = nn.Linear(7*7*16, 32)\n",
    "        self.l2 = nn.Linear(32, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.c1(x))\n",
    "        h = self.p1(h)\n",
    "        h = F.relu(self.c2(h))\n",
    "        h = self.p2(h)\n",
    "        h = h.view(-1, 7*7*16)\n",
    "        h = F.relu(self.l1(h))\n",
    "        h = F.relu(self.l2(h))\n",
    "        y = F.softmax(h, dim=1)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = Dataset_MNIST('./smalldataset/', mode='train', classes=[5,6,8], transform=torchvision.transforms.ToTensor(), balance=[0.8,0,0.2])\n",
    "dataset_test = Dataset_MNIST('./smalldataset/', mode='test', classes=[5,6,8], transform=torchvision.transforms.ToTensor(), balance=[0.8,0,0.2])\n",
    "dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_train_acc():\n",
    "    c = 0 \n",
    "    w = 0\n",
    "    for x, y in dataset_train:\n",
    "        if model(x[None,...].cuda()).argmax()==y:\n",
    "            c += 1\n",
    "        else:\n",
    "            w -= -1\n",
    "#     print('Train accuracy: {}'.format(c/(c+w)))\n",
    "    return c/(c+w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_test_acc():\n",
    "    c = 0 \n",
    "    w = 0\n",
    "    for x, y in dataset_test:\n",
    "        if model(x[None,...].cuda()).argmax()==y:\n",
    "            c += 1\n",
    "        else:\n",
    "            w -= -1\n",
    "#     print('Test accuracy: {}'.format(c/(c+w)))\n",
    "    return c/(c+w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial: 0, Epoch: 99, Loss: 1.8774224519729614\n",
      "Trial: 1, Epoch: 99, Loss: 1.7645620107650757\n",
      "Trial: 2, Epoch: 99, Loss: 2.2159297466278076\n",
      "Trial: 3, Epoch: 99, Loss: 1.8177797794342048\n",
      "Trial: 4, Epoch: 99, Loss: 1.7801299095153809\n",
      "Trial: 5, Epoch: 99, Loss: 1.7747573852539062\n",
      "Trial: 6, Epoch: 99, Loss: 1.5371670722961426\n",
      "Trial: 7, Epoch: 99, Loss: 1.6625707149505615\n",
      "Trial: 8, Epoch: 99, Loss: 1.5527881383895874\n",
      "Trial: 9, Epoch: 99, Loss: 1.7262291908264166\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "for trial in range(TRIALS):\n",
    "    criterion = nn.CrossEntropyLoss().cuda()\n",
    "    model = BasicModel().cuda()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=LR, momentum=0.9)\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda epoch: 0.999**epoch)\n",
    "    \n",
    "    weights = []\n",
    "    for child in model.children():\n",
    "        try:\n",
    "            weights.append(child.weight)\n",
    "            weights.append(child.bias)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    best_loss = 1000\n",
    "    for epoch in range(MAX_EPOCH):\n",
    "        for x, y in dataloader_train:\n",
    "    #         zero buffers\n",
    "            optimizer.zero_grad() \n",
    "    #         forward propagation\n",
    "            prediction = model(x.cuda())\n",
    "\n",
    "            loss = criterion(prediction, y.cuda())+sum([torch.abs(w).sum() for w in weights])*0.001\n",
    "\n",
    "    #         gradient calculation\n",
    "            loss.backward()\n",
    "    #         parameter update\n",
    "            optimizer.step() \n",
    "\n",
    "        print('Trial: {}, Epoch: {}, Loss: {}'.format(trial, epoch, loss), end='\\r')\n",
    "        scheduler.step()\n",
    "        \n",
    "        if loss<best_loss:\n",
    "            best_loss = loss\n",
    "            \n",
    "    losses.append(loss)\n",
    "    train_acc.append(calc_train_acc())\n",
    "    test_acc.append(calc_test_acc())\n",
    "    print()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Loss: 1.565, Mean Train Acc: 0.8514, Mean Test Acc: 0.7722\n"
     ]
    }
   ],
   "source": [
    "print('Mean Loss: {:.4}, Mean Train Acc: {:.4}, Mean Test Acc: {:.4}'.format(\n",
    "                                                        sum(losses)/TRIALS,\n",
    "                                                        sum(train_acc)/TRIALS,\n",
    "                                                        sum(test_acc)/TRIALS,\n",
    "                                                        ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
