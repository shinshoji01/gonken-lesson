{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For English course students :\n",
    "\n",
    "・Since the Japanese sentence and the English one represent the same content, it is sufficient to read the English.\n",
    "\n",
    "・I'm not good at English, so I apologize if I made any grammar or other mistakes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Readings:\n",
    "- [Grid search](https://note.com/okonomiyaki011/n/n5fb0365b5141)\n",
    "- [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)\n",
    "- [RandomezedSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)\n",
    "- [Bayesian optimization](https://qiita.com/masasora/items/cc2f10cb79f8c0a6bbaa)\n",
    "- [BayesSearchCV](https://scikit-optimize.github.io/stable/modules/generated/skopt.BayesSearchCV.html)\n",
    "- [OPTUNA](https://tech.preferred.jp/ja/blog/optuna-release/)\n",
    "- [OPTUNA tutorial](https://optuna.readthedocs.io/en/stable/tutorial/001_first.html#first)\n",
    "- [vcopt](https://vigne-cla.com/vcopt-specification/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## Overview\n",
    "1. What is a hyperparameter ?\n",
    "1. Introduction to hyperparameter optimization methods\n",
    "    1. Grid search\n",
    "    1. Random search\n",
    "    1. Bayesian optimization\n",
    "    1. GA\n",
    "1. Exercise\n",
    "1. Additional Problems "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# What is a hyperparameter?\n",
    "A hyperparameter is a parameter that controls the behavior of a machine learning algorithm. Especially in deep learning, they correspond to parameters that cannot be optimized by the gradient method. For example, things like learning rate, batch size, and number of learning iterations are hyperparameters. Also, the number of layers and channels in the neural network, and the choice of whether to use Momentum SGD or Adam for training are also hyperparameters.\n",
    "\n",
    "Adjusting hyperparameters is almost essential for a machine learning algorithm to perform well. In particular, deep learning tends to have a large number of hyperparameters, and their tuning has a large impact on performance.\n",
    "\n",
    "--\n",
    "\n",
    "# ハイパーパラメータとは？\n",
    "ハイパーパラメータとは、機械学習アルゴリズムの挙動を制御するパラメータのこと。特に深層学習では勾配法によって最適化できないパラメータに相当する。例えば、学習率やバッチサイズ、学習イテレーション数といったようなものがハイパーパラメータである。また、ニューラルネットワークの層数やチャンネル数、学習に Momentum SGD を用いるかそれとも Adam を用いるか、といったような選択もハイパーパラメータである。\n",
    "\n",
    "ハイパーパラメータの調整は機械学習アルゴリズムが力を発揮するためにほぼ不可欠である。特に、深層学習はハイパーパラメータの数が多い傾向がある上に、その調整が性能を大きく左右する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Introduction to hyperparameter optimization methods\n",
    "\n",
    "In this material, I will introduce typical hyperparameter optimization methods and hyperparameter optimization libraries.\n",
    "I will also introduce the concept of Pareto optimal solution, which represents the trade-off relationship in multi-objective optimization.\n",
    "\n",
    "--\n",
    "\n",
    "# ハイパーパラメータ最適化の手法紹介\n",
    "\n",
    "今回は、ハイパーパラメータの最適化手法の代表的なものの紹介と、ハイパーパラメータ最適化ライブラリの紹介、\n",
    "及びパレート最適解と言う、多目的最適化に於いてトレードオフの関係を表す概念を紹介する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Grid Search\n",
    "As the name implies, grid search is an exploration method that explores the points of a grid, and tries combinations of all values of hyperparameters with each hyperparameter value as a discrete value.\n",
    "The problem with grid search is that when the hyperparameters take discrete values as values, it becomes impossible to find the global optimal solution when there is a global optimal solution between the grid points. In addition, since all combinations of hyperparameter values are tested, the exploration time is generally longer than the other exploration methods mentioned here. However, if the hyperparameter values are only discrete values and the number of combinations is not very large, the exploration time can be very small.\n",
    "\n",
    "[Advantages]\n",
    "\n",
    "Useful when the value to be adjusted is known (ex: when the value takes discrete values)\n",
    "\n",
    "It is also useful when the number of values to be adjusted is small.\n",
    "\n",
    "[Disadvantages]\n",
    "\n",
    "More time is needed because of the increase the number of model training (42768 evaluations are required even for 11×2×12×3×3×3×2×3)\n",
    "\n",
    "Computational cost is very high.\n",
    "\n",
    "The following figure shows the image of grid search.\n",
    "\n",
    "--\n",
    "\n",
    "## グリッドサーチ\n",
    "まず、グリッドサーチであるが、その名の通り、格子点の点を探査していく探査手法で、各ハイパーパラメータの値を離散値として全ての値のハイパーパラメータの値の組み合わせを試すものである。\n",
    "グリッドサーチの問題点としては、ハイパーパラメータが値として離散値を取る場合に、格子点と格子点の間に大域最適解が存在する場合に、大域最適解を見つけることができなくなってしまうという問題がある。また、全てのハイパーパラメータの値の組み合わせを試す為、一般的には今回挙げた他の探査手法よりも探査時間がかかってしまうという問題点がある。しかしながら、ハイパーパラメータの値として離散値しか取らず、組み合わせの数もそれほど多くないような場合は探査時間は非常に少なくて済む。\n",
    "\n",
    "【メリット】\n",
    "\n",
    "調整する値のあたりが付いている場合に有用 (値が離散値を取る場合等)\n",
    "\n",
    "調整する値の数が少ない場合にも有用\n",
    "\n",
    "【デメリット】\n",
    "\n",
    "モデル訓練回数が増えるので時間が掛かる(11×2×12×3×3×3×2×3程度でも42768回の評価が必要)\n",
    "\n",
    "計算コストが非常に高い\n",
    "\n",
    "次図はグリッドサーチのイメージである。\n",
    "\n",
    "<img src=\"implements_13/gridsearch.png\" width=\"350\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple implementation that is easy to understand is to execute the for statement multiple times as follows.\n",
    "\n",
    "--\n",
    "\n",
    "感覚的に理解しやすい簡単な実装としては次のようにfor文をひたすら回す形となる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11, 2, 12, 1, 3, 3, 3, 2, 3, 1]\n"
     ]
    }
   ],
   "source": [
    "def calc_score(param01, param02, param03, param04, param05, param06, param07, param08, param09, param10):\n",
    "    #Write process to calculate the score\n",
    "    result = param01 + param02 + param03 + param04 + param05 + param06 + param07 + param08 + param09 + param10\n",
    "    return result\n",
    "\n",
    "param_dist = {\"param01\":[1,2,3,4,5,6,7,8,9,10,11],\n",
    "\t              \"param02\":[1,2],\n",
    "\t              \"param03\":[1,2,3,4,5,6,7,8,9,10,11,12],\n",
    "\t              \"param04\":[1],\n",
    "\t              \"param05\":[1,2,3],\n",
    "\t              \"param06\":[1,2,3],\n",
    "\t              \"param07\":[1,2,3],\n",
    "\t              \"param08\":[1,2],\n",
    "\t              \"param09\":[1,2,3],\n",
    "\t\t\t\t  \"param10\":[1]\n",
    "\t              }\n",
    "\n",
    "score = 0\n",
    "params = []\n",
    "[params.append(0) for i in range(10)]\n",
    "\n",
    "for a in param_dist[\"param01\"]:\n",
    "    param01 = a\n",
    "    for b in param_dist[\"param02\"]:\n",
    "        param02 = b\n",
    "        for c in param_dist[\"param03\"]:\n",
    "            param03 = c\n",
    "            for d in param_dist[\"param04\"]:\n",
    "                param04 = d\n",
    "                for e in param_dist[\"param05\"]:\n",
    "                    param05 = e\n",
    "                    for f in param_dist[\"param06\"]:\n",
    "                        param06 = f\n",
    "                        for g in param_dist[\"param07\"]:\n",
    "                            param07 = g\n",
    "                            for h in param_dist[\"param08\"]:\n",
    "                                param08 = h\n",
    "                                for i in param_dist[\"param09\"]:\n",
    "                                    param09 = i\n",
    "                                    for j in param_dist[\"param10\"]:\n",
    "                                        param10 = j\n",
    "                                        t = calc_score(param01, param02, param03, param04, param05, param06, param07, param08, param09, param10)\n",
    "                                        \n",
    "                                        if t > score:\n",
    "                                            score = t\n",
    "                                            params[0] = param01\n",
    "                                            params[1] = param02\n",
    "                                            params[2] = param03\n",
    "                                            params[3] = param04\n",
    "                                            params[4] = param05\n",
    "                                            params[5] = param06\n",
    "                                            params[6] = param07\n",
    "                                            params[7] = param08\n",
    "                                            params[8] = param09\n",
    "                                            params[9] = param10\n",
    "\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a library called gridsearchCV. It automatically does the cross-validation and evaluation for you.\n",
    "\n",
    "- [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)\n",
    "\n",
    "fit(), predict(), score(), get_params(), set_params()\n",
    "\n",
    "↑ These functions must be implemented with exact match names.\n",
    "\n",
    "--\n",
    "\n",
    "ライブラリとしてはgridsearchCVというものがある。これは交差検証まで自動で行い、評価を行ってくれるものである。\n",
    "\n",
    "- [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)\n",
    "\n",
    "fit(), predict(), score(), get_params(), set_params()\n",
    "\n",
    "↑これらの関数は完全一致の関数名で必ず実装する必要がある。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
      "0          0.001500      0.000500         0.064462        0.005496   \n",
      "1          0.001499      0.000499         0.092447        0.001499   \n",
      "2          0.001000      0.000000         0.071459        0.008497   \n",
      "3          0.001498      0.000498         0.073959        0.024984   \n",
      "4          0.000498      0.000498         0.072957        0.001001   \n",
      "...             ...           ...              ...             ...   \n",
      "6907       0.001000      0.000002         0.029982        0.002000   \n",
      "6908       0.000500      0.000500         0.031483        0.001498   \n",
      "6909       0.000500      0.000500         0.032483        0.002498   \n",
      "6910       0.000499      0.000499         0.041984        0.002990   \n",
      "6911       0.001500      0.000502         0.061463        0.014493   \n",
      "\n",
      "     param_param01 param_param02 param_param03 param_param04 param_param05  \\\n",
      "0                1             1             1             1             1   \n",
      "1                1             1             1             1             1   \n",
      "2                1             1             1             1             1   \n",
      "3                1             1             1             1             1   \n",
      "4                1             1             1             1             1   \n",
      "...            ...           ...           ...           ...           ...   \n",
      "6907             3             4             4             3             4   \n",
      "6908             3             4             4             3             4   \n",
      "6909             3             4             4             3             4   \n",
      "6910             3             4             4             3             4   \n",
      "6911             3             4             4             3             4   \n",
      "\n",
      "     param_param06 param_param07  \\\n",
      "0                1             1   \n",
      "1                1             2   \n",
      "2                1             3   \n",
      "3                2             1   \n",
      "4                2             2   \n",
      "...            ...           ...   \n",
      "6907             3             2   \n",
      "6908             3             3   \n",
      "6909             4             1   \n",
      "6910             4             2   \n",
      "6911             4             3   \n",
      "\n",
      "                                                 params  split0_test_score  \\\n",
      "0     {'param01': 1, 'param02': 1, 'param03': 1, 'pa...           1.752875   \n",
      "1     {'param01': 1, 'param02': 1, 'param03': 1, 'pa...           0.375941   \n",
      "2     {'param01': 1, 'param02': 1, 'param03': 1, 'pa...           1.471899   \n",
      "3     {'param01': 1, 'param02': 1, 'param03': 1, 'pa...           0.025740   \n",
      "4     {'param01': 1, 'param02': 1, 'param03': 1, 'pa...           1.980193   \n",
      "...                                                 ...                ...   \n",
      "6907  {'param01': 3, 'param02': 4, 'param03': 4, 'pa...           0.137943   \n",
      "6908  {'param01': 3, 'param02': 4, 'param03': 4, 'pa...           0.980842   \n",
      "6909  {'param01': 3, 'param02': 4, 'param03': 4, 'pa...           0.595147   \n",
      "6910  {'param01': 3, 'param02': 4, 'param03': 4, 'pa...           0.771089   \n",
      "6911  {'param01': 3, 'param02': 4, 'param03': 4, 'pa...           0.309011   \n",
      "\n",
      "      split1_test_score  mean_test_score  std_test_score  rank_test_score  \n",
      "0              1.752875         1.752875    0.000000e+00              384  \n",
      "1              0.375941         0.375941    0.000000e+00             4964  \n",
      "2              1.471899         1.471899    2.220446e-16              870  \n",
      "3              0.025740         0.025740    3.469447e-18             6768  \n",
      "4              1.980193         1.980193    0.000000e+00               24  \n",
      "...                 ...              ...             ...              ...  \n",
      "6907           0.137943         0.137943    0.000000e+00             6192  \n",
      "6908           0.980842         0.980842    0.000000e+00             1762  \n",
      "6909           0.595147         0.595147    0.000000e+00             3819  \n",
      "6910           0.771089         0.771089    0.000000e+00             2877  \n",
      "6911           0.309011         0.309011    0.000000e+00             5319  \n",
      "\n",
      "[6912 rows x 17 columns]\n",
      "##################################\n",
      "{'param01': 1, 'param02': 4, 'param03': 4, 'param04': 3, 'param05': 2, 'param06': 3, 'param07': 1}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class MyEstimator(BaseEstimator):\n",
    "    def __init__(self, param01, param02, param03, param04, param05, param06, param07):\n",
    "        self.param01 = int(param01)\n",
    "        self.param02 = int(param02)\n",
    "        self.param03 = int(param03)\n",
    "        self.param04 = int(param04)\n",
    "        self.param05 = int(param05)\n",
    "        self.param06 = int(param06)\n",
    "        self.param07 = int(param07)\n",
    "        \n",
    "        self.make_string(self.param01, self.param02, self.param03, self.param04,self.param05, self.param06, self.param07)\n",
    "        self.df = pd.read_csv(CSV_PATH, header=0)\n",
    "    \n",
    "    def make_string(self, param01, param02, param03, param04, param05, param06, param07):\n",
    "        #param1\n",
    "        noiseReduction = [\"Gaussian:7 \",\"Gaussian:7 Gaussian:7 \",\"Gaussian:7 Gaussian:7 Gaussian:7 \"]\n",
    "        #param2\n",
    "        filtersize_1 = [\"3 \",\"5 \",\"7 \",\"9 \"]\n",
    "        #param3\n",
    "        filtersize_2 = [\"3 \",\"5 \",\"7 \",\"9 \"]\n",
    "        #param4\n",
    "        padding = [\"1 \",\"3 \",\"5 \"]\n",
    "        #param5\n",
    "        activation_function_1 = [\"softmax \",\"relu \",\"sigmoid \",\"tanh \"]\n",
    "        #param6\n",
    "        activation_function_2 = [\"softmax \",\"relu \",\"sigmoid \",\"tanh \"]\n",
    "        #param7\n",
    "        optimization = [\"Adam\",\"RMSProp\",\"AdaGrad\"]\n",
    "        \n",
    "        self.param01_str = noiseReduction[param01 - 1]\n",
    "        self.param02_str = filtersize_1[param02 - 1]\n",
    "        self.param03_str = filtersize_2[param03 - 1]\n",
    "        self.param04_str = padding[param04 - 1]\n",
    "        self.param05_str = activation_function_1[param05 - 1]\n",
    "        self.param06_str = activation_function_2[param06 - 1]\n",
    "        self.param07_str = optimization[param07 - 1]\n",
    "        \n",
    "        self.search_str = (self.param01_str + self.param02_str + self.param03_str + self.param04_str + self.param05_str + self.param06_str + self.param07_str)\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        return self \n",
    "    \n",
    "    def predict(self, x):\n",
    "        return [1.0]*len(x)\n",
    "    \n",
    "    def score(self, x, y):\n",
    "        self.make_string(self.param01, self.param02, self.param03, self.param04, self.param05, self.param06, self.param07)\n",
    "        judge_value = float(MyEstimator(self.param01, self.param02, self.param03, self.param04, self.param05, self.param06, self.param07).search_data())\n",
    "        return judge_value\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        return {'param01': self.param01, 'param02': self.param02, 'param03': self.param03, 'param04': self.param04, 'param05': self.param05, 'param06': self.param06, 'param07': self.param07}\n",
    "                \n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self\n",
    "    \n",
    "    def search_data(self):\n",
    "        self.make_string(self.param01, self.param02, self.param03, self.param04, self.param05, self.param06, self.param07)\n",
    "        \n",
    "        return float(self.df[self.df['param'] == self.search_str]['Evaluation Value'])\n",
    "\n",
    "param01 = 1\n",
    "param02 = 1\n",
    "param03 = 1\n",
    "param04 = 1\n",
    "param05 = 1\n",
    "param06 = 1\n",
    "param07 = 1\n",
    "CSV_PATH  = \"data_13/data.csv\"\n",
    "\n",
    "searchEstimator = MyEstimator(param01, param02, param03, param04, param05, param06, param07)\n",
    "value = searchEstimator.search_data()\n",
    "\n",
    "param_dist = {\"param01\":[1,2,3],\n",
    "              \"param02\":[1,2,3,4],\n",
    "              \"param03\":[1,2,3,4],\n",
    "              \"param04\":[1,2,3],\n",
    "              \"param05\":[1,2,3,4],\n",
    "              \"param06\":[1,2,3,4],\n",
    "              \"param07\":[1,2,3],\n",
    "              }\n",
    "\n",
    "model_grid = GridSearchCV(  estimator = searchEstimator, \n",
    "                                    param_grid = param_dist,\n",
    "                                    cv=2,              #CV default=None クロスバリデーションの分割方法を決定\n",
    "                                    #n_iter = 1,        #何パターンまでrando searchで調べるかの指定が必要 default:10\n",
    "                                    #scoring=\"accuracy\",#metrics モデル評価ルールを記述する\n",
    "                                    return_train_score=False,\n",
    "                                    n_jobs=-1           #num of core -1は全てのコアを利用\n",
    "                                    #verbose=0,          \n",
    "                                    #random_state=2, #乱数のSEEDを指定\n",
    "                                    #return_train_score = True #スコアを返すか返さないか\n",
    "                                    #scoring = 'roc_auc'\n",
    "                                    #scoring = scoring\n",
    "                                    ,refit=True\n",
    "                                    )\n",
    "\n",
    "x = searchEstimator.df[\"param\"]\n",
    "y = searchEstimator.df[\"Evaluation Value\"].values\n",
    "\n",
    "model_grid.fit(x,y)\n",
    "result_df = pd.DataFrame(model_grid.cv_results_)\n",
    "print(result_df)\n",
    "print(\"##################################\")\n",
    "print(model_grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___ \n",
    "## Random Search\n",
    "Next is random search, which is a method to randomly determine the value of each hyperparameter and use it to find the most accurate combination of hyperparameter values. The values of the hyperparameters are determined randomly and then evaluated.\n",
    "In a random search, whether or not the best hyperparameter values are found is a matter of \"luck\". However, by retrying a combination that has been tried once, it is generally possible to find the global optimum solution faster than with grid search. Also, if the hyperparameters can take discrete values, it has the advantage of being able to find the global optimum solution between grid points, which is difficult to find with grid search.\n",
    "\n",
    "The following figure shows the image of random search.\n",
    "\n",
    "--\n",
    "\n",
    "## ランダムサーチ\n",
    "続いてランダムサーチであるが、これは各ハイパーパラメータの値をランダムに決定し、それを用いて最も精度の良いハイパーパラメータの値の組み合わせを探す手法である。ランダムにハイパーパラメータの値を決定し、評価していく。\n",
    "ランダムサーチでは最適なハイパーパラメータの値が見つかるかどうかは\"運\"である。しかしながら、一度試した組み合わせが出た場合はリトライする等の工夫を加えることで、一般的にはグリッドサーチよりも早く大域最適解を見つることができる。また、ハイパーパラメータが離散値を取りうる場合は、グリッドサーチでは発見困難であった格子点と格子点の間の大域最適解を見つけることが可能となるという利点がある、\n",
    "\n",
    "\n",
    "\n",
    "次図はランダムサーチのイメージである。\n",
    "\n",
    "<img src=\"implements_13/randomsearch.png\" width=\"350\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple implementation that is easy to understand is to use the rand() function to set parameter values and repeat the process over and over again, as shown below.\n",
    "\n",
    "--\n",
    "\n",
    "感覚的に理解しやすい簡単な実装としては、次のようにrand()関数でパラメータの値を設定し、それを何度も繰り返す形となる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 1, 4, 1, 3, 1, 2, 2, 3, 1]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def calc_score(param01, param02, param03, param04, param05, param06, param07, param08, param09, param10):\n",
    "    #Write process to calculate the score\n",
    "    result = param01 + param02 + param03 + param04 + param05 + param06 + param07 + param08 + param09 + param10\n",
    "    return result\n",
    "\n",
    "param_dist = {\"param01\":[1,2,3,4,5,6,7,8,9,10,11],\n",
    "\t              \"param02\":[1,2],\n",
    "\t              \"param03\":[1,2,3,4,5,6,7,8,9,10,11,12],\n",
    "\t              \"param04\":[1],\n",
    "\t              \"param05\":[1,2,3],\n",
    "\t              \"param06\":[1,2,3],\n",
    "\t              \"param07\":[1,2,3],\n",
    "\t              \"param08\":[1,2],\n",
    "\t              \"param09\":[1,2,3],\n",
    "\t\t\t\t  \"param10\":[1]\n",
    "\t              }\n",
    "\n",
    "score = 0\n",
    "\n",
    "for a in range(40000):\n",
    "    param01 = random.randint(1,11)\n",
    "    param02 = random.randint(1,2)\n",
    "    param03 = random.randint(1,12)\n",
    "    param04 = 1\n",
    "    param05 = random.randint(1,3)\n",
    "    param06 = random.randint(1,3)\n",
    "    param07 = random.randint(1,3)\n",
    "    param08 = random.randint(1,2)\n",
    "    param09 = random.randint(1,3)\n",
    "    param10 = 1\n",
    "    \n",
    "    t = calc_score(param01, param02, param03, param04, param05, param06, param07, param08, param09, param10)\n",
    "                                        \n",
    "    if t > score:\n",
    "        t = score\n",
    "        params[0] = param01\n",
    "        params[1] = param02\n",
    "        params[2] = param03\n",
    "        params[3] = param04\n",
    "        params[4] = param05\n",
    "        params[5] = param06\n",
    "        params[6] = param07\n",
    "        params[7] = param08\n",
    "        params[8] = param09\n",
    "        params[9] = param10\n",
    "        \n",
    "print(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a library called RandomizedSearchCV, which automatically does the cross-validation and evaluation for you.\n",
    "\n",
    "- [RandomezedSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)\n",
    "\n",
    "Like a gridsearchCV, \n",
    "\n",
    "fit(), predict(), score(), get_params(), set_params()\n",
    "\n",
    "↑ These functions must be implemented with exact match names.\n",
    "\n",
    "--\n",
    "\n",
    "ライブラリとしてはRandomizedSearchCVというものがあるが、これは交差検証まで自動で行い、評価を行ってくれるものである。\n",
    "\n",
    "- [RandomezedSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)\n",
    "\n",
    "gridsearchCV同様に\n",
    "\n",
    "fit(), predict(), score(), get_params(), set_params()\n",
    "\n",
    "関数は完全一致の関数名で必ず実装する必要がある。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   mean_fit_time  std_fit_time  mean_score_time  std_score_time param_param07  \\\n",
      "0       0.001332  4.693475e-04         0.039312        0.007360             3   \n",
      "1       0.000666  4.712019e-04         0.031982        0.002449             1   \n",
      "2       0.000999  1.184119e-06         0.035980        0.003265             2   \n",
      "3       0.001000  5.840039e-07         0.037646        0.003088             3   \n",
      "4       0.001000  8.104673e-07         0.036644        0.001248             3   \n",
      "5       0.001000  8.778064e-07         0.038644        0.008803             3   \n",
      "6       0.001333  4.714828e-04         0.054302        0.003297             3   \n",
      "7       0.000667  4.713150e-04         0.038310        0.004781             2   \n",
      "8       0.002665  1.699160e-03         0.050971        0.004318             1   \n",
      "9       0.001001  6.861413e-06         0.037642        0.003859             2   \n",
      "\n",
      "  param_param06 param_param05 param_param04 param_param03 param_param02  \\\n",
      "0             2             1             3             2             3   \n",
      "1             3             2             1             1             3   \n",
      "2             4             1             2             2             1   \n",
      "3             4             2             3             1             2   \n",
      "4             4             2             3             4             1   \n",
      "5             4             1             1             1             1   \n",
      "6             2             2             3             2             3   \n",
      "7             2             2             2             1             3   \n",
      "8             2             1             3             1             4   \n",
      "9             4             3             2             3             1   \n",
      "\n",
      "  param_param01                                             params  \\\n",
      "0             2  {'param07': 3, 'param06': 2, 'param05': 1, 'pa...   \n",
      "1             3  {'param07': 1, 'param06': 3, 'param05': 2, 'pa...   \n",
      "2             3  {'param07': 2, 'param06': 4, 'param05': 1, 'pa...   \n",
      "3             3  {'param07': 3, 'param06': 4, 'param05': 2, 'pa...   \n",
      "4             2  {'param07': 3, 'param06': 4, 'param05': 2, 'pa...   \n",
      "5             3  {'param07': 3, 'param06': 4, 'param05': 1, 'pa...   \n",
      "6             2  {'param07': 3, 'param06': 2, 'param05': 2, 'pa...   \n",
      "7             2  {'param07': 2, 'param06': 2, 'param05': 2, 'pa...   \n",
      "8             2  {'param07': 1, 'param06': 2, 'param05': 1, 'pa...   \n",
      "9             2  {'param07': 2, 'param06': 4, 'param05': 3, 'pa...   \n",
      "\n",
      "   split0_test_score  split1_test_score  split2_test_score  mean_test_score  \\\n",
      "0           1.010648           1.010648           1.010648         1.010648   \n",
      "1           0.094517           0.094517           0.094517         0.094517   \n",
      "2           0.811996           0.811996           0.811996         0.811996   \n",
      "3           0.233360           0.233360           0.233360         0.233360   \n",
      "4           0.390077           0.390077           0.390077         0.390077   \n",
      "5           1.315772           1.315772           1.315772         1.315772   \n",
      "6           0.348243           0.348243           0.348243         0.348243   \n",
      "7           0.397142           0.397142           0.397142         0.397142   \n",
      "8           0.322149           0.322149           0.322149         0.322149   \n",
      "9           0.276747           0.276747           0.276747         0.276747   \n",
      "\n",
      "   std_test_score  rank_test_score  \n",
      "0    0.000000e+00                2  \n",
      "1    1.387779e-17               10  \n",
      "2    0.000000e+00                3  \n",
      "3    0.000000e+00                9  \n",
      "4    5.551115e-17                5  \n",
      "5    0.000000e+00                1  \n",
      "6    5.551115e-17                6  \n",
      "7    0.000000e+00                4  \n",
      "8    0.000000e+00                7  \n",
      "9    0.000000e+00                8  \n",
      "##################################\n",
      "{'param07': 3, 'param06': 4, 'param05': 1, 'param04': 1, 'param03': 1, 'param02': 1, 'param01': 3}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "class MyEstimator(BaseEstimator):\n",
    "    def __init__(self, param01, param02, param03, param04, param05, param06, param07):\n",
    "        self.param01 = int(param01)\n",
    "        self.param02 = int(param02)\n",
    "        self.param03 = int(param03)\n",
    "        self.param04 = int(param04)\n",
    "        self.param05 = int(param05)\n",
    "        self.param06 = int(param06)\n",
    "        self.param07 = int(param07)\n",
    "        \n",
    "        self.make_string(self.param01, self.param02, self.param03, self.param04,self.param05, self.param06, self.param07)\n",
    "        self.df = pd.read_csv(CSV_PATH, header=0)\n",
    "    \n",
    "    def make_string(self, param01, param02, param03, param04, param05, param06, param07):\n",
    "        #param1\n",
    "        noiseReduction = [\"Gaussian:7 \",\"Gaussian:7 Gaussian:7 \",\"Gaussian:7 Gaussian:7 Gaussian:7 \"]\n",
    "        #param2\n",
    "        filtersize_1 = [\"3 \",\"5 \",\"7 \",\"9 \"]\n",
    "        #param3\n",
    "        filtersize_2 = [\"3 \",\"5 \",\"7 \",\"9 \"]\n",
    "        #param4\n",
    "        padding = [\"1 \",\"3 \",\"5 \"]\n",
    "        #param5\n",
    "        activation_function_1 = [\"softmax \",\"relu \",\"sigmoid \",\"tanh \"]\n",
    "        #param6\n",
    "        activation_function_2 = [\"softmax \",\"relu \",\"sigmoid \",\"tanh \"]\n",
    "        #param7\n",
    "        optimization = [\"Adam\",\"RMSProp\",\"AdaGrad\"]\n",
    "        \n",
    "        self.param01_str = noiseReduction[param01 - 1]\n",
    "        self.param02_str = filtersize_1[param02 - 1]\n",
    "        self.param03_str = filtersize_2[param03 - 1]\n",
    "        self.param04_str = padding[param04 - 1]\n",
    "        self.param05_str = activation_function_1[param05 - 1]\n",
    "        self.param06_str = activation_function_2[param06 - 1]\n",
    "        self.param07_str = optimization[param07 - 1]\n",
    "        \n",
    "        self.search_str = (self.param01_str + self.param02_str + self.param03_str + self.param04_str + self.param05_str + self.param06_str + self.param07_str)\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        return self \n",
    "    \n",
    "    def predict(self, x):\n",
    "        return [1.0]*len(x)\n",
    "    \n",
    "    def score(self, x, y):\n",
    "        self.make_string(self.param01, self.param02, self.param03, self.param04, self.param05, self.param06, self.param07)\n",
    "        judge_value = float(MyEstimator(self.param01, self.param02, self.param03, self.param04, self.param05, self.param06, self.param07).search_data())\n",
    "        return judge_value\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        return {'param01': self.param01, 'param02': self.param02, 'param03': self.param03, 'param04': self.param04, 'param05': self.param05, 'param06': self.param06, 'param07': self.param07}\n",
    "                \n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self\n",
    "    \n",
    "    def search_data(self):\n",
    "        self.make_string(self.param01, self.param02, self.param03, self.param04, self.param05, self.param06, self.param07)\n",
    "        \n",
    "        return float(self.df[self.df['param'] == self.search_str]['Evaluation Value'])\n",
    "\n",
    "param01 = 1\n",
    "param02 = 1\n",
    "param03 = 1\n",
    "param04 = 1\n",
    "param05 = 1\n",
    "param06 = 1\n",
    "param07 = 1\n",
    "CSV_PATH  = \"data_13/data.csv\"\n",
    "\n",
    "searchEstimator = MyEstimator(param01, param02, param03, param04, param05, param06, param07)\n",
    "value = searchEstimator.search_data()\n",
    "\n",
    "param_dist = {\"param01\":[1,2,3],\n",
    "              \"param02\":[1,2,3,4],\n",
    "              \"param03\":[1,2,3,4],\n",
    "              \"param04\":[1,2,3],\n",
    "              \"param05\":[1,2,3,4],\n",
    "              \"param06\":[1,2,3,4],\n",
    "              \"param07\":[1,2,3],\n",
    "              }\n",
    "\n",
    "model_random = RandomizedSearchCV(  estimator = searchEstimator, \n",
    "                                    param_distributions = param_dist,\n",
    "                                    #cv=3,              #CV default=None クロスバリデーションの分割方法を決定\n",
    "                                    n_iter = 10,        #何パターンまでrando searchで調べるか指定 default:10\n",
    "                                    #scoring=\"accuracy\",#metrics モデル評価ルールを記述する。\n",
    "                                    n_jobs=-1           #num of core -1は全てのコアを利用\n",
    "                                    #verbose=0,          \n",
    "                                    #random_state=2, #乱数seed\n",
    "                                    #return_train_score = True #スコアを返すか返さないか\n",
    "                                    #scoring = 'roc_auc'\n",
    "                                    #scoring = scoring\n",
    "                                    ,refit = False\n",
    "                                    )\n",
    "\n",
    "\n",
    "x = searchEstimator.df[\"param\"]\n",
    "y = searchEstimator.df[\"Evaluation Value\"].values\n",
    "model_random.fit(x,y)\n",
    "result_df = pd.DataFrame(model_random.cv_results_)\n",
    "\n",
    "print(result_df)\n",
    "print(\"##################################\")\n",
    "print(model_random.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Bayesian Optimization\n",
    "Next is Bayesian estimation, which is a type of optimization algorithm that uses uncertainty to find the next value to search for. A Gaussian process is used as a proxy model to estimate the objective function.\n",
    "\n",
    "Simply put, it is a method to determine the next value to search based on the previous result. It is similar to a human treasure hunt.\n",
    "\n",
    "In Bayesian optimization, optimization is done sequentially using two strategies, \"Exploitation\" and \"Utilization\". \"Exploitation\" means that when a good result is obtained, we continue to investigate its vicinity.\n",
    "On the other hand, \"Exploitation\" is a strategy to investigate different areas far away from the current position, thinking that there is a better combination than the current one. If the results are not satisfactory, you can use the rand() function to go to a different location.\n",
    "\n",
    "In this way, Bayesian optimization allows us to try the next step in a balanced manner based on the previous result.\n",
    "\n",
    "\n",
    "If I were to explain the detailed theory, I would not be able to finish in this time, so I will skip it. If you want to know more, please refer to the following URL.\n",
    "\n",
    "- [Bayesian optimization](https://qiita.com/masasora/items/cc2f10cb79f8c0a6bbaa)\n",
    "\n",
    "Please keep in mind that this is a way to use the previous results to gradually move in the optimal direction.\n",
    "\n",
    "--\n",
    "\n",
    "## ベイズ最適\n",
    "次にベイズ推定であるが、これは不確かさを利用して次に探索を行うべき値を探していく最適化アルゴリズムの一種である。目的関数を推定する代理モデルにガウス過程が使われる。\n",
    "\n",
    "簡単に言えば、前回の結果を基に次に調べる値を決める手法。人間が宝探しをするような感覚に近い。\n",
    "\n",
    "ベイズ最適化では\"探索\"と\"活用\"の2つの戦略を使って最適化を順次的に行う。\"活用\"とは、良い結果が出た場合は継続してその近辺を調べることである。\n",
    "一方、\"探索\"は現在の位置よりも、もっと良い組み合わせがあると考えてあえて現在位置から離れた異なる部分を調べる戦略である。イメージ的には、ある部分を探査していて、探査結果が思わしくないようであれば、rand()関数を用いて現在位置とは別の場所へ探査へ行くイメージである。\n",
    "\n",
    "このようにベイズ最適化では前回の結果を踏まえて次をバランス良く試すことが可能である。\n",
    "\n",
    "\n",
    "細かな理論まで説明するとこの時間では終わらない為、割愛する。詳しく知りたい人は次のURLを参照して下さい。\n",
    "\n",
    "- [Bayesian optimization](https://qiita.com/masasora/items/cc2f10cb79f8c0a6bbaa)\n",
    "\n",
    "前の結果を利用して、最適な方向へ徐々に進んでいく方法なんだなくらいにつかんでおいて下さい。\n",
    "\n",
    "<img src=\"implements_13/bayesian0.jpg\" width=\"350\">\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"implements_13/bayesian.jpg\" width=\"350\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a library called BayesSearchCV and Optuna.\n",
    "\n",
    "- [BayesSearchCV](https://scikit-optimize.github.io/stable/modules/generated/skopt.BayesSearchCV.html)\n",
    "\n",
    "- [Optuna tutorial](https://optuna.readthedocs.io/en/stable/tutorial/001_first.html#first)\n",
    "\n",
    "In this time, I will show an example of BayesSearchCV. I have been implemented Optuna before, so if you would like to see an example of an implementation using optuna, please tell me.\n",
    "\n",
    "--\n",
    "\n",
    "ライブラリとしてはBayesSearchCVやoptunaというものがある。\n",
    "\n",
    "- [BayesSearchCV](https://scikit-optimize.github.io/stable/modules/generated/skopt.BayesSearchCV.html)\n",
    "\n",
    "- [Optuna tutorial](https://optuna.readthedocs.io/en/stable/tutorial/001_first.html#first)\n",
    "\n",
    "今回は、BayesSearchCVを例示する。optunaの実装経験はあるので、もし、optunaを用いた実装例が見たい方は声をかけて下さい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   split0_test_score  split1_test_score  mean_test_score  std_test_score  \\\n",
      "0           0.135945           0.135945         0.135945    0.000000e+00   \n",
      "1           1.884874           1.884874         1.884874    0.000000e+00   \n",
      "2           0.319997           0.319997         0.319997    5.551115e-17   \n",
      "3           0.769721           0.769721         0.769721    0.000000e+00   \n",
      "4           0.015238           0.015238         0.015238    0.000000e+00   \n",
      "5           0.316085           0.316085         0.316085    0.000000e+00   \n",
      "6           0.949880           0.949880         0.949880    0.000000e+00   \n",
      "7           0.230639           0.230639         0.230639    0.000000e+00   \n",
      "8           0.538090           0.538090         0.538090    0.000000e+00   \n",
      "9           0.818719           0.818719         0.818719    0.000000e+00   \n",
      "\n",
      "   rank_test_score  mean_fit_time  std_fit_time  mean_score_time  \\\n",
      "0                9       0.000998  4.768372e-07         0.026485   \n",
      "1                1       0.001000  4.768372e-07         0.028483   \n",
      "2                6       0.000000  0.000000e+00         0.027011   \n",
      "3                4       0.001001  1.311302e-06         0.025974   \n",
      "4               10       0.000509  5.093813e-04         0.025485   \n",
      "5                7       0.000499  4.988909e-04         0.025962   \n",
      "6                2       0.000998  7.152557e-07         0.028494   \n",
      "7                8       0.000500  4.996061e-04         0.028985   \n",
      "8                5       0.000988  1.144409e-05         0.026508   \n",
      "9                3       0.000999  5.960464e-07         0.034979   \n",
      "\n",
      "   std_score_time  param_param01  param_param02  param_param03  param_param04  \\\n",
      "0        0.002499              2              3              3              1   \n",
      "1        0.003499              3              4              4              3   \n",
      "2        0.003994              1              2              2              1   \n",
      "3        0.002008              3              4              3              3   \n",
      "4        0.003498              3              4              2              1   \n",
      "5        0.002025              3              2              1              1   \n",
      "6        0.004488              2              4              3              1   \n",
      "7        0.004996              3              1              3              2   \n",
      "8        0.004476              2              1              1              2   \n",
      "9        0.004998              1              3              4              3   \n",
      "\n",
      "   param_param05  param_param06  param_param07  \\\n",
      "0              3              4              2   \n",
      "1              3              1              1   \n",
      "2              1              2              3   \n",
      "3              2              3              3   \n",
      "4              4              4              3   \n",
      "5              2              2              2   \n",
      "6              3              1              3   \n",
      "7              2              2              3   \n",
      "8              3              4              3   \n",
      "9              2              1              3   \n",
      "\n",
      "                                              params  \n",
      "0  {'param01': 2, 'param02': 3, 'param03': 3, 'pa...  \n",
      "1  {'param01': 3, 'param02': 4, 'param03': 4, 'pa...  \n",
      "2  {'param01': 1, 'param02': 2, 'param03': 2, 'pa...  \n",
      "3  {'param01': 3, 'param02': 4, 'param03': 3, 'pa...  \n",
      "4  {'param01': 3, 'param02': 4, 'param03': 2, 'pa...  \n",
      "5  {'param01': 3, 'param02': 2, 'param03': 1, 'pa...  \n",
      "6  {'param01': 2, 'param02': 4, 'param03': 3, 'pa...  \n",
      "7  {'param01': 3, 'param02': 1, 'param03': 3, 'pa...  \n",
      "8  {'param01': 2, 'param02': 1, 'param03': 1, 'pa...  \n",
      "9  {'param01': 1, 'param02': 3, 'param03': 4, 'pa...  \n",
      "##################################\n",
      "OrderedDict([('param01', 3), ('param02', 4), ('param03', 4), ('param04', 3), ('param05', 3), ('param06', 1), ('param07', 1)])\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from skopt import BayesSearchCV\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "warnings.simplefilter('ignore', UserWarning)\n",
    "\n",
    "class MyEstimator(BaseEstimator):\n",
    "    def __init__(self, param01, param02, param03, param04, param05, param06, param07):\n",
    "        self.param01 = int(param01)\n",
    "        self.param02 = int(param02)\n",
    "        self.param03 = int(param03)\n",
    "        self.param04 = int(param04)\n",
    "        self.param05 = int(param05)\n",
    "        self.param06 = int(param06)\n",
    "        self.param07 = int(param07)\n",
    "        \n",
    "        self.make_string(self.param01, self.param02, self.param03, self.param04,self.param05, self.param06, self.param07)\n",
    "        self.df = pd.read_csv(CSV_PATH, header=0)\n",
    "    \n",
    "    def make_string(self, param01, param02, param03, param04, param05, param06, param07):\n",
    "        #param1\n",
    "        noiseReduction = [\"Gaussian:7 \",\"Gaussian:7 Gaussian:7 \",\"Gaussian:7 Gaussian:7 Gaussian:7 \"]\n",
    "        #param2\n",
    "        filtersize_1 = [\"3 \",\"5 \",\"7 \",\"9 \"]\n",
    "        #param3\n",
    "        filtersize_2 = [\"3 \",\"5 \",\"7 \",\"9 \"]\n",
    "        #param4\n",
    "        padding = [\"1 \",\"3 \",\"5 \"]\n",
    "        #param5\n",
    "        activation_function_1 = [\"softmax \",\"relu \",\"sigmoid \",\"tanh \"]\n",
    "        #param6\n",
    "        activation_function_2 = [\"softmax \",\"relu \",\"sigmoid \",\"tanh \"]\n",
    "        #param7\n",
    "        optimization = [\"Adam\",\"RMSProp\",\"AdaGrad\"]\n",
    "        \n",
    "        self.param01_str = noiseReduction[param01 - 1]\n",
    "        self.param02_str = filtersize_1[param02 - 1]\n",
    "        self.param03_str = filtersize_2[param03 - 1]\n",
    "        self.param04_str = padding[param04 - 1]\n",
    "        self.param05_str = activation_function_1[param05 - 1]\n",
    "        self.param06_str = activation_function_2[param06 - 1]\n",
    "        self.param07_str = optimization[param07 - 1]\n",
    "        \n",
    "        self.search_str = (self.param01_str + self.param02_str + self.param03_str + self.param04_str + self.param05_str + self.param06_str + self.param07_str)\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        return self \n",
    "    \n",
    "    def predict(self, x):\n",
    "        return [1.0]*len(x)\n",
    "    \n",
    "    def score(self, x, y):\n",
    "        self.make_string(self.param01, self.param02, self.param03, self.param04, self.param05, self.param06, self.param07)\n",
    "        judge_value = float(MyEstimator(self.param01, self.param02, self.param03, self.param04, self.param05, self.param06, self.param07).search_data())\n",
    "        return judge_value\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        return {'param01': self.param01, 'param02': self.param02, 'param03': self.param03, 'param04': self.param04, 'param05': self.param05, 'param06': self.param06, 'param07': self.param07}\n",
    "                \n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self\n",
    "    \n",
    "    def search_data(self):\n",
    "        self.make_string(self.param01, self.param02, self.param03, self.param04, self.param05, self.param06, self.param07)\n",
    "        \n",
    "        return float(self.df[self.df['param'] == self.search_str]['Evaluation Value'])\n",
    "    \n",
    "param01 = 1\n",
    "param02 = 1\n",
    "param03 = 1\n",
    "param04 = 1\n",
    "param05 = 1\n",
    "param06 = 1\n",
    "param07 = 1\n",
    "CSV_PATH  = \"data_13/data.csv\"\n",
    "\n",
    "bayesEstimator = MyEstimator(param01, param02, param03, param04, param05, param06, param07)\n",
    "value = bayesEstimator.search_data()\n",
    "\n",
    "param_dist = {\"param01\":[1,2,3],\n",
    "              \"param02\":[1,2,3,4],\n",
    "              \"param03\":[1,2,3,4],\n",
    "              \"param04\":[1,2,3],\n",
    "              \"param05\":[1,2,3,4],\n",
    "              \"param06\":[1,2,3,4],\n",
    "              \"param07\":[1,2,3],\n",
    "              }\n",
    "\n",
    "model_bayes = BayesSearchCV(estimator = bayesEstimator,\n",
    "                                #param_distributions = param_dist,\n",
    "                                search_spaces = param_dist,\n",
    "                                cv = 2,              #CV default=None クロスバリデーションの分割方法を決定\n",
    "                                n_iter = 10,         #おおよそ22回まででサンプリング点が収束した為、この値とした。\n",
    "                                #何パターンまで調べるかの指定が必要 default:50(BayesSearchCVの場合) \n",
    "                                #interation numサンプリングされるパラメータ設定の数。 \n",
    "                                #n_iterは実行時間とソリューションの品質をトレードオフにします。\n",
    "                                #scoring=\"accuracy\", #metrics モデル評価ルールを記述する。\n",
    "                                n_jobs= -1,           #num of core -1は全てのコアを利用\n",
    "                                #verbose=0,          \n",
    "                                #random_state=2, #乱数seed\n",
    "                                #return_train_score = True #スコアを返すか返さないか\n",
    "                                #scoring = 'roc_auc'\n",
    "                                #scoring = scoring\n",
    "                                #search_spaces=param_grid,\n",
    "                                #refit=True\n",
    "                                #fit_params = pass_params\n",
    "                                )\n",
    "\n",
    "x = bayesEstimator.df[\"param\"]\n",
    "y = bayesEstimator.df[\"Evaluation Value\"].values\n",
    "\n",
    "\n",
    "model_bayes.fit(x,y)\n",
    "result_df = pd.DataFrame(model_bayes.cv_results_)\n",
    "\n",
    "print(result_df)\n",
    "print(\"##################################\")\n",
    "print(model_bayes.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Optimization by GA\n",
    "\n",
    "Finally, Optimization by GA. This is a method that uses a genetic algorithm to search for the best combination of hyperparameters. First, genes are generated randomly, and the ones with the best accuracy are passed on to the next generation. The best combination of hyperparameters is found through mutation and crossover.\n",
    "\n",
    "My impression from practical use is that if the hyperparameters are discrete values, it is possible to find the ones with good accuracy very fast.\n",
    "\n",
    "There is a library called vcopt developed by a startup from Tokyo Institute of Technology\n",
    "\n",
    "--\n",
    "\n",
    "## GAによる最適化\n",
    "\n",
    "最後にGAによる最適解であるが、これは遺伝的アルゴリズムを利用し、最良のハイパーパラメータの組み合わせを探査する手法である。まずはランダムに遺伝子を生成し、その中から精度の良いものを次の世代へ受け継ぐ。その中で突然変異や、交叉等を行い、ハイパーパラメータの組み合わせとして最良のものを探し出す。\n",
    "\n",
    "私が実務で使ったなりの感想ではあるが、ハイパーパラメータが値として離散値を取る場合は、非常に高速に、精度の良いものを見つけることができる印象であった。\n",
    "\n",
    "ライブラリとしては東工大発のベンチャーが開発したvcoptというものがある。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________ info ________________________________________\n",
      "para_range     : n=7\n",
      "score_func     : <class 'function'>\n",
      "aim            : ==9999.0\n",
      "show_pool_func : 'data/'\n",
      "seed           : None\n",
      "pool_num       : 100\n",
      "max_gen        : 2000\n",
      "core_num       : 1 (*vcopt, vc-grendel)\n",
      "_______________________________________ start ________________________________________\n",
      "Scoring first gen 100/100        \n",
      "|                                       +<                                        | gen=21, best_score=1.9998\n",
      "_______________________________________ result _______________________________________\n",
      "para = np.array([1, 4, 4, 3, 2, 3, 1])\n",
      "score = 1.99977001\n",
      "________________________________________ end _________________________________________\n",
      "[1 4 4 3 2 3 1] 1.99977001\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import pandas as pd\n",
    "from vcopt import vcopt\n",
    "import time\n",
    "\n",
    "#評価関数\n",
    "def score_func(params):\n",
    "    search_str = make_string_for_GA(params[0],params[1],params[2],\n",
    "                                    params[3],params[4],params[5],params[6])\n",
    "    \n",
    "    judge_value = float(df[df['param'] == search_str]['Evaluation Value'])\n",
    "\n",
    "    return judge_value\n",
    "\n",
    "def show_pool_func(pool, **info):\n",
    "    #GA中の諸情報はinfoという辞書に格納されて渡される\n",
    "    #これらを受け取って使用することができる\n",
    "    gen = info['gen'] #現在の世代\n",
    "    best_index = info['best_index'] #エリート個体のインデックス\n",
    "    best_score = info['best_score'] #エリート個体の評価値\n",
    "    mean_score = info['mean_score'] #個体群の平均評価値\n",
    "    mean_gap = info['mean_gap'] #目標値と評価値の差の絶対値平均\n",
    "    time = info['time'] #経過時間（秒）\n",
    "\n",
    "    #可視化\n",
    "    print(gen, best_score, best_index, time)\n",
    "\n",
    "def make_string_for_GA(param01, param02, param03, param04, param05, param06, param07):\n",
    "    #param1\n",
    "    noiseReduction = [\"Gaussian:7 \",\"Gaussian:7 Gaussian:7 \",\"Gaussian:7 Gaussian:7 Gaussian:7 \"]\n",
    "    #param2\n",
    "    filtersize_1 = [\"3 \",\"5 \",\"7 \",\"9 \"]\n",
    "    #param3\n",
    "    filtersize_2 = [\"3 \",\"5 \",\"7 \",\"9 \"]\n",
    "    #param4\n",
    "    padding = [\"1 \",\"3 \",\"5 \"]\n",
    "    #param5\n",
    "    activation_function_1 = [\"softmax \",\"relu \",\"sigmoid \",\"tanh \"]\n",
    "    #param6\n",
    "    activation_function_2 = [\"softmax \",\"relu \",\"sigmoid \",\"tanh \"]\n",
    "    #param7\n",
    "    optimization = [\"Adam\",\"RMSProp\",\"AdaGrad\"]\n",
    "\n",
    "    param01_str = noiseReduction[param01 - 1]\n",
    "    param02_str = filtersize_1[param02 - 1]\n",
    "    param03_str = filtersize_2[param03 - 1]\n",
    "    param04_str = padding[param04 - 1]\n",
    "    param05_str = activation_function_1[param05 - 1]\n",
    "    param06_str = activation_function_2[param06 - 1]\n",
    "    param07_str = optimization[param07 - 1]\n",
    "\n",
    "    return (param01_str + param02_str + param03_str + param04_str + param05_str + param06_str + param07_str)\n",
    "\n",
    "param01 = 1\n",
    "param02 = 1\n",
    "param03 = 1\n",
    "param04 = 1\n",
    "param05 = 1\n",
    "param06 = 1\n",
    "param07 = 1\n",
    "CSV_PATH  = \"data_13/data.csv\"\n",
    "\n",
    "# データ生成と配列の確保\n",
    "df = pd.read_csv(CSV_PATH, header=0)\n",
    "dataX = df[\"param\"]\n",
    "dataY = df[\"Evaluation Value\"].values\n",
    "\n",
    "target_str = make_string_for_GA(param01, param02, param03, param04, param05, param06, param07)\n",
    "target_value = float(df[df['param'] == target_str]['Evaluation Value'])\n",
    "\n",
    "search_str = \"\"\n",
    "param_range = [[1,2,3], [2,3,4], [1,2,3,4], [1,2,3], [1,2,3,4], [1,2,3,4], [1,2,3]]\n",
    "\n",
    "para, score = vcopt().dcGA(param_range ,\n",
    "                               score_func,\n",
    "                               9999, #最大化\n",
    "                               show_pool_func = 'data/',\n",
    "                               seed = None, #乱数seedを指定\n",
    "                               pool_num = 100 #個体数を指定\n",
    "                               ,max_gen = 2000 #最大世代数を指定\n",
    "                               )\n",
    "\n",
    "print(para, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Exercise \n",
    "(I thought you would be very busy with your graduation thesis, so I reduced the number of questions this time.)\n",
    "\n",
    "(1) Write the most appropriate optimizing method for hyperparameters in the following cases.\n",
    "\n",
    "1-1 When the hyperparameter takes discrete values as values and the number of combinations is not so large.\n",
    "\n",
    "1-2 When the hyperparameter takes a continuous value as a value and the number of combinations is very large. (There is more than one answer, so please write the one you like.)\n",
    "\n",
    "(2) Describe your research briefly, and write whether you think one of the methods listed in this lecture is appropriate for optimizing hyperparameters in your research, including reasons.\n",
    "\n",
    "--\n",
    "\n",
    "## 練習問題 \n",
    "(卒業論文で非常に忙しいと思いましたので、今回は問題数を減らしました。)\n",
    "\n",
    "(1) 次の場合に、ハイパーパラメータの探査手法として最も適切なものを書け。\n",
    "\n",
    "1-1 ハイパーパラメータが値として離散値を取り、組み合わせ数もそれほど多くない場合。\n",
    "\n",
    "1-2 ハイパーパラメータが値として連続値を取り、組み合わせの数が非常に多い場合。(解答は一つではないので、好きなものを書いて下さい。)\n",
    "\n",
    "(2) あなたの研究内容を簡単に記述し、あなたの研究でハイパーパラメータの最適化を行う際に今回挙げた中では手法が適切と考えられるか、理由も含めて書け。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Problems \n",
    "(This does not affect the your score, but if you are free, please work on it.)\n",
    "\n",
    "(3) Add learning iteration as a hyperparameter to the code of GridSearchCV.\n",
    "\n",
    "(4) Add learning iteration as a hyperparameter to the code of RandmizedSearchCV.\n",
    "\n",
    "(5) Add learning iteration as a hyperparameter to the code of Baysian optimization.\n",
    "\n",
    "(6) Add learning iteration as a hyperparameter to the code of GA.\n",
    "\n",
    "\n",
    "(The dataset does not contain learning iteration, so you do not need to run.)\n",
    "\n",
    "--\n",
    "\n",
    "## 追加問題 \n",
    "(評価に影響はありませんが、もし、物足りないという方が居たら取り組んでみて下さい。)\n",
    "\n",
    "(3) GridSearchCVのコードに学習イテレーションをハイパーパラメータとして加えよ。\n",
    "\n",
    "(4) RandmizedSearchCVのコードに学習イテレーションをハイパーパラメータとして加えよ。\n",
    "\n",
    "(5) Baysian optimizationのコードに学習イテレーションをハイパーパラメータとして加えよ。\n",
    "\n",
    "(6) GAのコードに学習イテレーションをハイパーパラメータとして加えよ。\n",
    "\n",
    "\n",
    "(データセットには学習イテレーションの項目は含まれていないので、実行はしなくて大丈夫です。)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
